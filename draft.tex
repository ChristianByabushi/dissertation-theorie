\documentclass[12pt,a4paper, oneside]{book} 
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} 
\usepackage[english]{babel} 
\usepackage{amsmath} 
\usepackage{amsfonts}
\usepackage{lscape} 
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{longtable}
\usepackage{array,tabularx}
\usepackage{graphicx}
%\usepackage{apacite} 
\usepackage{geometry}
%\usepackage[round, sort]{natbib}
\usepackage{latexsym}
\usepackage[xindy]{glossaries}
\usepackage{multirow}
\usepackage{soul}
\usepackage{listings}
\usepackage{pdfpages}
\usepackage{makeidx}
\usepackage{graphics} 
\usepackage{array} 
\usepackage{booktabs}
\usepackage{listings}  
\usepackage{minted} 
%%\usepackage[table]{xcolor} 
\usepackage{hyperref}  
\usepackage{enumitem} 
\usepackage{csquotes} 
\usepackage[labelfont=bf]{caption} 
\usepackage{xcolor} 
\usepackage{multirow} 
\usepackage{multicol}
\usepackage{color}
\usepackage[xindy]{glossaries}
\usepackage{multirow}
\usepackage{soul}
\usepackage{pdfpages}
\usepackage{makeidx}
\usepackage{colortbl}
\usepackage{setspace}
\parindent=0cm
\usepackage{colortbl}
\usepackage{setspace} 
\usepackage{subcaption}
\hypersetup{colorlinks=true, linkcolor=blue, citecolor=blue, filecolor=blue, urlcolor=blue}
\geometry{left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}
\parindent=0cm

\definecolor{darkgreen}{RGB}{0, 128, 64}
\definecolor{dark}{RGB}{35, 35, 35}
\definecolor{orange}{RGB}{255, 69, 0}


\lstdefinestyle{mystyle}{
	language=Python,
	basicstyle=\ttfamily,
	keywordstyle=\color{blue},
	commentstyle=\color{darkgreen},
	stringstyle=\color{dark},
	numbers=left,
	frame=single,
	moredelim=**[is][\color{orange}]{@}{@},
}
\renewcommand{\lstlistingname}{Code Example} 
\definecolor{myblack}{rgb}{0.1, 0.1, 0}
\lstdefinestyle{stylepython}{
	backgroundcolor=\color{blue!8},  % Background color (adjust to your desired color)
	basicstyle=\ttfamily,            % Font style
	language=Python,                % Language (e.g., Python)
	numbers=left,                   % Line numbers on the left
	numberstyle=\tiny\color{myblack},  % Line number style
	breaklines=true,                % Automatically wrap long lines
	frame=single,                   % Add a frame around the code
	showstringspaces=false,         % Don't show spaces in strings
	keywordstyle=\color{myblack},      % Keyword color
	commentstyle=\color{myblack},     % Comment color
	stringstyle=\color{myblack}         % String color
}


%\renewcommand{\thepart}{\roman{part}}
\usemintedstyle{default}
% Define a style for Python code
\lstset{
	language=Python,
	basicstyle=\ttfamily\small, % Font size and style
	keywordstyle=\color{blue}, % Keywords font color
	stringstyle=\color{orange}, % Strings font color
	commentstyle=\color{gray}, % Comments font color
	showstringspaces=false, % Don't show spaces in strings
	breaklines=true, % Wrap long lines
	numbers=left, % Show line numbers
	numberstyle=\tiny\color{gray}, % Line numbers font style
	frame=single, % Add a frame around the code
} 

\definecolor{codebackground}{RGB}{240, 240, 240}
\definecolor{codecomments}{RGB}{0, 128, 0}

\lstdefinestyle{stylejupyter}{
	language=Python,
	backgroundcolor=\color{codebackground},
	commentstyle=\color{codecomments},
	basicstyle=\small\ttfamily,
	frame=single,
	rulecolor=\color{codebackground},
	keywordstyle=\color{blue},
	numbers=left,
	numberstyle=\tiny\color{gray},
	stringstyle=\color{magenta},
	breaklines=true,
	showstringspaces=false,
	escapeinside={(*@}{@*)},  % Escape to LaTeX inside code comments
	morekeywords={as, np, pd, plt},  % Add custom keywords here
}

\definecolor{codebackground}{RGB}{240, 240, 240}
\definecolor{codecomments}{RGB}{0, 128, 0}

\lstdefinestyle{htmlcssjsstyle}{
	backgroundcolor=\color{codebackground},
	basicstyle=\small\ttfamily,
	frame=single,
	rulecolor=\color{codebackground},
	keywordstyle=\color{blue},
	commentstyle=\color{codecomments},
	numbers=left,
	numberstyle=\tiny\color{gray},
	stringstyle=\color{magenta},
	breaklines=true,
	showstringspaces=false,
	escapeinside={(*@}{@*)},  % Escape to LaTeX inside code comments
	morekeywords={div, span, h1, h2, p, a, class, id, var, function, let, const},  % Add custom keywords here
}


\geometry{left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm} 

\begin{document} 
	
	\begin{titlepage}
		\centering \Large \textbf{CATHOLIC UNIVERSITY OF BUKAVU}\\
		\begin{center}
			\begin{figure}[h]
				\centering
				\includegraphics[width=4cm, height=4cm]{"../../../../Latex Projects/ucb"}
			\end{figure}
			\large{B.P.285 Bukavu}
		\end{center}
		\vspace*{\stretch{0.5}}
		\begin{center}
			\huge{\large{Faculty of sciences  }} \\
			{\Large \hspace*{0.7cm} Department of Computer Science}  
		\end{center}
		\vspace*{\stretch{1}}
		\hrulefill
		\begin{center}\bfseries\Large
			\textcolor{blue}{Development of an interface application for detection of spam on a mobile operator: Case study of Airtel, Vodacom and Orange.}\normalsize
		\end{center}
		\hrulefill \\
		
		\vspace{1\baselineskip}
		\normalsize 
			\begin {minipage}{0.5 \textwidth }	
		\begin{flushright}
			{
				\vspace {0.1cm} 
				\begin{tabbing}					
					\hspace*{3.4cm}Presented by : \normalsize{MURHULA BYABUSHI Christian}  \\
					\\
					\hspace{3.4cm}\textit{Dissertation presented and defended in order to obtain the}\\
					\hspace{3.4cm}\textit{degree of Bachelor in Computer Science.}\\
					\vspace*{0.1cm}\\
					\hspace*{3.4					
						cm}Option: Network and Telecommunications\\					
					\hspace*{3.4cm}Degree: Final year of Bachelor\\ 
					\\
					\hspace*{3.4cm}Supervised by: Hw. MUGISHO MUSHEGERHA Youen \\
					\hspace*{3.4cm}Directed by : PhD. Elie ZIHINDULA
						
				\end{tabbing} 
			
			}					
			
		\end{flushright}
	\end{minipage}

		\begin{center} 
			\vspace*{1cm}
			\LARGE\textbf{Academic year: 2022-2023}
		\end{center}   
	\end{titlepage}   
\frontmatter
   \chapter{Epigraph}
	
	\chapter{In memoriam} 
	
	
	\chapter{Dedicated} 
	
	
	\chapter{Acknowledgments} 
	
	
 
	\tableofcontents
	\listoffigures

	\listoftables

	\mainmatter

	\chapter*{Introduction}  
	\pagestyle{plain}
	\addcontentsline{toc}{chapter}{Introduction}
	\section{Context and generalities} 
	With the increasing of use of mobile devices in mobile telecommunication, the number of text messages sent every day has grown exponentially. According to \textit{Statista}, a company that provides market and consumer data on a wide range of topics, including digital media and technology; the number of mobile messages sent worldwide in 2020 reached 3.5 trillion \cite{Statista2020}. \\
	
	In the same case, with the raise of web pages and social media messaging applications like Whatshap, Telgram, Snapchat Facebook, Instagram and many others, phone users can now send messages  that are only based on text as in former time but also on video, audios which are more chestful comparatively \cite{faklaris2016oh}. \\
	
	For sure, for interacting with his partner more professionally, email message is the most mean used, but not in all cases, since in some countries a  people commonly use \textit{Sim card} delivered by a telecommunication provider to launch mobile bank services. Hence, it appears more valuable than only being a communication technology.\\
	
	Along with various services and interests that mobiles messages encompasses, from mobile baking to social media communications, games entertainment, localization platforms and many others the number of messages sent have been increase. However these includes the genre that aim to deceive people into providing personal information, fakes news, sending unwillingly money, menacing to death or taking other actions that benefit the scammer.\\
		
	To address this problem, the development of an interface application with spam detection capabilities appears crucial, since it allows on a side:The setting and filtering of messages in applications, and the delivering to the phone user an assistance helping him to take better decisions. On the other side, providing to networks operators a further solution worthy to be encompassed into the messages transmitters systems for more transactions monitoring.
	%such as : Airtel, Vodacom and Orange. \\
	% Indeed, Airtel, Vodacom, and Orange are major telecommunications operators which mostly function in Democratic Republic of Congo, with a significant market share and importance in online services, including messaging. 
	 
	\section{Problematic} 
	In telecommunication area, it is used mobile devices or phones for sharing either texts or emails and chats by using targeted applications. Among all, it is used specifically \textit{SMS} for sharing personal and professional information  \cite{lavanya2582sms}. The \textit{SMS} stands for Short Message Service, which is a text messaging service for mobile phones. At the beginning it was allowing users to send messages  least than 16O characters \cite{le2005mobile}. However in today world, it is possible to send more than that even multimedia types. Their sources can be either human or other phones users, online applications or networks operators \cite{jangir2016design}.\\
		
	From time to time, a certain group of people benefit from this accessibility, liberality and send messages which are insane or completely unwilling by receivers. Their genre looks like this: "You won x amount of money send another amount to withdraw it", "Join me at x area to take your money but pay me the transport ...", "I'm \textit{Sirene} Madam I have money for you", "I have a job for you" and many others fake messages. More of them are reported in this  \href{ https://blog.textedly.com/spam-text-message-examples}{spams examples link}.  \\
		
	Furthermore, other kinds of scammers even arrive at stage where they utilize the vulnerabilities which are present in messaging system, and inject, or even install mobile spyware users systems. For instance we have the \textit{SimJacker} referenced by \cite{cimpanu2019simjacker}. This shows that the threats can even come not only from messages content but from even its system. \\
	
	Speaking about unwilling messages involves to investigate too, in the common organizations systems which afford their market plan or advertisement by paying networks operators. Therefore, scammers use that mean to appears too as authorized organization providing to users different services. Hence, tacking an official and nonofficial becomes challenging\cite{chen2017survey},\cite{leppaniemi2008mobile}.\\
		
	Considering all issues and challenges mentioned above in which spams messages are implicated, it becomes urgent to find solution(s) helping or supporting all victims. But, how can these challenges including false market advertising messages to threats and unwilling messages, be tackled for facilitating a proper messaging communication ?	
	\section{Hypotheses}  
	 According to the \textit{Oxford} dictionary, hypotheses stands for a statement of the expected relationship between things being studied, which is intended to explain certain facts or observations \cite{park2012dictionary}. An idea to be tested.
	 Hence, using the content-based filtering techniques, which involves analyzing the content of messages and determining whether it is a spam or not would be considered as solution. \\
	 
	 Actually, the solution at the mentioned issue involves the entire participation of victims and the systems, in other words human and human participation. Hence, it seems effective to use new technologies that can give capacity to systems to detect by themselves the threats and warns the users. This is possible by the modern techniques called : Machine learning algorithms.\\
 
	 For the benefit of this work, it is utilized several types of those new technologies encompassing algorithms which are: \textbf{Naive Bayes, Logistic Regression, and Supper vector Machines} helping to assistant the users. Since they have different performances, this work combines their capacities the other techniques called : Ensemble models allowing to get one result which can help the phone to interact with the user \cite{raschka2017python}. In their logic, they include functions allowing them to classify after being prepared and trained whether a message is legitimate or not \cite{karl1986model}. \\
	 	 
	 Furthermore, the solution involves other techniques means helping the phone system to interact with the others online services providing to it a support internally so that it may know how to filter its inputs, whether they contain the threats or not. The third party contacted gives details about the message content and the \textbf{probability} to trust it.\\
	 	 
	 Overall, tackling those issues leads to build a smart system which assists and provides the user with informations that can increase his capacity of judging the. That should start from the development of models which feeds data to its deployment in softwares ares allowing him to consume the model's services \cite{hadullo2021machine}.
	 	 
	\section{Delimitation and objectives}  
	\subsection{Delimitation}
	The present work aims to develop a messaging application for communication and detection of spam on a mobile network.
	\\
	
	Geographically it focuses on all provinces of Democratic Republic Of Congo(DRC) where mobile phones are used and require techniques for implementation. \\
		
	The solution provided is limited in terms of landscapes where it can be utilized since it's smartly function only on topics(data) on which it is concerned. As a result, it is challenging to claim its effectiveness only in languages like Swahili, French, and English. \\
		
	Indeed, the current work in terms of planning and execution has spanned a duration of nine months : From January to November 2023. 
	\subsection{Objectives}
	The current work compromises 2 types of objectives which are: functional and non-functional.\\
		
	As functional objectives, it yields a interface application system that can interact with messaging applications and facilitates efficient communication between users; by implementing the machine learning models which having the capacity of classifying messages and preventing unwilling messages under a certain probability.\\
				
	Concerning non-functional objectives, the current work allows users whose messaging applications complies with relevant privacy laws and regulation, to protect data information, reducing the attacks and frauds; increasing the trustfulness of other users. Consequently, it increases the credibility in networks operators companies services that gain another tool helpful in surveillance and monitoring. 
	
	\section{Interest} 
	Personally, the current work has allowed the authors to gain knowledge and more experience in the field of mobile networks and messaging applications trough the research investigated in. \\
	
	In the society, it is worthy to contribute to facilitating communication and reducing the impact  of spam messages, which are somehow annoying and stressful for citizens.\\
		
	Economically, it is appears as time saver in business employees by decreasing concerns based on threats and unwilling messages due to its capacity of providing filtering functions.
			
	For scientists, this work is a reference for those whose to delve in and gaining skills and techniques in mobile networks environment in machine learning operations systems.
	
	\section{Research Methodology}
	Throughout this paper, the research methodology is used to guide the study towards achieving its objectives. It adopts a descriptive research design to describe the development of a interface application for better communication and detection of spam on a mobile network. The study will focus on both qualitative and quantitative research methods \cite{creswell2014research}. The qualitative method  involves a focus on literature review, interviews and analysis of collected messages, while the quantitative method focuses on development and testing of the interface messaging application.\\
		
	The current research has been conducted in two phases. The first one involves data collection through a survey questionnaire form that is completed on website. The others involves the downloading of data from online working community. Once collected, they are analyzed using descriptive statistics \cite{bluman2017elementary} including the techniques of identifying the common types of messages and the frequency of occurrence and languages within it, etc.  \\
	
	The second phase involves the development of the pursuing of steps offered by the machine learning operating systems (MLOps) encompassing data prepocessing, wrangling, feature engineering, splitting for model training till it deployment for being integrated in a system. 
	By following the workflows stages delivered by the MlOps for production models, it covers the architecture of consumming machine learning services trough a software system compromises the programming technologies like Python with its framework Django and HTML, Css, and JavaScript for interaction with it.
	
	The evaluation of the messaging application interface is conducted using both quantitative and qualitative methods. In fact, the qualitative evaluation involves the measurement of the application's accuracy and efficiency for detection and filtering messages while the qualitative evaluates user stability and application's experience.
	
	\section{Work Plan (or Work Subdivision)}
	The work plan of this dissertation is divided into four parts. The first is the introduction, which provides a background information on the research problem.The second part consist of a situation analysis and assessment while the third part focuses on literature review and explanations on the methodology. Then the fourth part presents the practical result of this work. Finally the conclusion part summarizes the key findings and contributions of the study and presents limitations and provide recommendations for future research. \\
		
	\chapter{Situation analysis and assessment on mobile phones}
	\section{Introduction}
	In this chapter, we will focus on various aspects that enhance the comprehensiveness and practicality of this dissertation. It includes explanations of mobile messaging architecture, machine learning models, and spam messages in mobile world. Additionally, it provides an analysis of the architecture used by network operators, highlighting both positive and negative aspects of their approach to message handling.
	\section{Presentation of the working framework and definition of key concepts}
	\subsection{Definition of key concepts}
	\begin{enumerate}[label=\alph*)]
		\item SMS(Short Message Service) :\\
		The Short Message Service is a basic service allowing the exchange of short text messages between subscribers \cite{mobilemessaging}. For supporting virtually all mobile devices, SMS is considered as a universal means of communication that enables users to communicate and function even though all users are not active simultaneously (asynchronous communication).
		\item Enhanced Messaging Service (EMS) : \\
		EMS has been created to allow the transmission of richer and more advanced messages. Unlike traditional SMS, EMS accepts not only text messages but also audios, melodies, and animations \cite{le2005mobile}.
	    
	    \item MMS (Multimedia Messaging Service):\\
	    MSS has been developed to facilitate the transmission of rich multimedia content in mobile messaging. Unlike SMS and EMS, MMS enables users to send not only text messages but also various types of multimedia files such as images, videos, audio recordings and even slideshows \cite{le2005mobile}.
	    \newpage
		\item Spam message: \\
		A spam message is understood as an unsolicited or undesired messages received on mobile phones which constitutes veritable nuisance to the mobile subscribers \cite{shafi2017review}. Clearly, this message can be sent with the intention of gaining financial benefits, collecting personal or organizational information such as security numbers, credit card details, or login credentials, and soliciting money by making false promises of future benefits or rewards that do not materialize. 
		
		\item Networks operators:
		The  networks operators refers to companies or organizations that provide and manage telecommunication networks. These operators own and operate the infrastructure, such as mobile networks, fixed-line networks, or internet service provider (ISP) networks, that enable the transmission of user's information to another user of the network \cite{ghezzi2015strategy} 
		
		\item Artificial Intelligence (AI) :
		 AI refers to the field of computer science that focuses on creating intelligent machines or systems that can perform tasks that would typically require human intelligence. For being practical, it encompasses algorithms, models and technologies that enable computers and machines to simulate human like cognitive processes such as learning, reasoning, problem-solving, perception and language understanding.  
		 
		 \item ML (Machine Learning)  : 
		 Machine learning is a subfield of Artificial Intelligence that focus on the development of algorithms and models that enable computers to learn from data and make decisions or predictions without being explicitly programmed\cite{smola2010introduction}. Clearly, when the data is labeled during the training, we refer to it as supervised model. If contrast, when the data is unlabeled and the model must discover patterns and relationships itself, it is an unsupervised model. Additionally, whenever it performs both the labeling  and discovering patterns tasks, it refers to a semi-supervised model. Furthermore, there is the last type called reinforcement. This one, is used to teach a computer or an AI agent how to make series of decisions in an environment. Just like, we learn to play the game better by playing it over and over.   
		 
		 \item NLP (Natural Language Process):
		 NLP is a subfield of Machine Learning that studies the human language and combing techniques from statistics, linguistics, life-hoods for making sentiment analysis, text classification, machine translation, question answering and text generation in a way that it can be understood computationally \cite{cambria2014jumping}. 
		  \end{enumerate}
		\subsection{Presentation of the working framework}  
		In the eastern party of DRC (South Kivu- and North Kivu) the usage of mobile phones has become more common, transforming communication and connectivity in the region. 
	The DRC itself is a large country, covering over 2,345,000 square kilometers with the eastern provinces of North and South Kivu spanning approximately 59,483 and 65,070 square kilometers respectively \cite{giswatch2018}. According to recent statistics from \textit{GlobalEdge} \footnote{ \href{https://globaledge.msu.edu/}{GlobalEdge}  : Created in 1994 by the International Business Center and the Eli Broad College of Business at Michigan State University (IBC), globalEDGE™ is a knowledge web-portal that connects international business professionals worldwide to a wealth of information, insights, and learning resources on global business activities}, an American company, around 95 million people were living in the DRC in 2022 \cite{monusco2015}, of which approximately 46.9\% had active mobile phones based on \textit{GSM} \footnote{\href{https://www.gsma.com/aboutus/}{GSMA}  (Global System Communications Association) : An industry Organization which represents the interests of mobile network operators worldwide created in 1982 to ease cooperation between countries deploying \textit{GSM} (Global System fo Mobile) technology.} research. \\ 
		
		In this context, it is observed that more people in cities use mobile phones compared to those in villages, primarily due to limited accessibility. A research study conducted by \textit{Target Canibet} \footnote{\href{https://www.target-sarl.cd/fr/content/etude-sur-la-telephonie-mobile-en-rdc}{Target Canibet: Reseach \& Consulting Group working in DRC. \url{https://www.target-sarl.cd/fr/content/etude-sur-la-telephonie-mobile-en-rdc} }}in 2015 focused on mobile connections in DRC cities including Bukavu, Goma, Kinshasa, Lubumbashi, and Matadi, found that among 1,000 people surveyed in each city, 9 out of 10 individuals were subscribed to a network operator. However, it was noted that approximately half of them subscribed to two operators, while a quarter subscribed to four operators, and 18\% used the services of a single operator.  \\
		
		Furthermore, the recent statistics made by \textit{DataReportal} \footnote{DataReportal: A online Company designed to help people and organizations all over the world to find the data, insights, and trends they need to make better informed decisions produced by Simon Kemp, \url{https://datareportal.com/reports/digital-2023-global-overview-report}}  in DRC shows that the mobiles users continues to increase exponentially, merely because of new services provided by internet and Telecoms Operators, at the point that since 2021 to 2022, it is has been reported 3.6 million of new users between 2021 to 2022, a report that proves how much mobile phones is inevitable in this last decades.
			
	   \subsection{Network coverage and infrastructure}   
	    In fact, two telecoms services exist in DRC such as : Fixed services (26\%) and Mobile services (74\%). The first one known as landline or wired services, involve the use of physical infrastructure; the second one which is popular is the mobiles services refer to telecommunications services provided through mobile networks.
	    According to the Congolese Regulatory Agency (ARPTC), the DRC has four mobile operators - Vodacom RDC,
	    Airtel Congo, Orange DRC and Africell DRC. Vodacom is the leader in the voice segment, with 35.2\% of the market,
	    followed by Orange (30\%), Airtel (23.9\%) and Africell (10.9\%). In the mobile internet market, Vodacom has 37.44\%, Airtel
	    31.25\%, Orange 28.14\% and Africell 3.17\% \cite{stateInternet2019}. \\
	     	     	        
	   Additionally, since the 190s, when the DRC witnessed the first installation of operator systems such as Celtel(now Airtel) and Vodacom, followed by Orange and Africell, the telecomunications sector has shown significant market growth, reaching 1 Billion in 2022\$ and expanding at a rate of 21\% per year according to  \textit{GlobalData} \footnote{\href{https://www.globaldata.com/store/report/drc-telecom-operators-market-analysis/}{GlobalData: Expert Company of Analysis, innovatove Solutions}}. \\
	   
	   However, this growth necessitates the updating of the infrastructure, which includes various generations of technologies, namely the second generation, third, fourth, and fifth(under development).\\
	   	   
	   In fact, the second generation have been deployed in various territories to enable more efficient \textbf{voice calls, data networks services, and introduce SMS for text messaging}. The infrastructure required for 2G networks includes the following equipments: 1) BTS(Base Transceiver Station) : Transmit and receives signals between mobile devices. 2) MSC(Mobile Switch Controller) : serves as the switching entity that connects calls between mobile devices. 3) BSC (Base Station Controller) :  manages multiples BTSs and controlling radio resources, managing handovers between cells and optimizing network performance, 4) AuC (Authentication Center) responsible for managing subscriber authentication and encryption keys to ensure communication between mobile devices and the network, 5) Home Location Register (HLR) the database that stores subscriber information such as phone numbers, authentication details, and service profiles, 6) Visitor Location Register (VLR) : The VlR is a temporary database that stores information about roaming subscribers within a specific area 7) MS (Mobile Station), including all the technologies used by the  users's handset and has two parts :\textbf{ Firstly, the mobile equipment which contains the radio equipment, the user interface, the processing capability and memory requirements for call signaling, encryption, SMS and the id of the mobile phone(equipment IMEI number). Secondly the Subscriber Identity module (SIM Card),} used in encryption of codes needed to identify the subscriber, storing subscriber's information, locate the user \cite{realWorldNetworks} as (+243 for each congolese number). \\
	   Indeed, all the 2G technologies covers a large distance varying between 1880MHz - 2700 MHz.\\
	      
	   Besides, the third generation appears as revolution, \textbf{allowing multimedia messages, voice calls data, faster data speed}; however it requires a significant upgrade from the previous generation. Thus, the equipment involved in 3G technology includes : 1) BTS (Base station Transceiver) : Which plays the same role as for 2G; 2) Node B: Responsible for handling the radio interface and connecting mobile devices to the core network; 3) Radio Network Controller (RNC) : Controlling the Node B and managing the radio resources 4) Mobile Switching Center (MSC): The MSC is the central switching entity in the network that \textbf{connects calls between mobile devices}; 5) Serving GPRS Support Node (SGSN) : Responsible for managing packet-switched data services services and handling mobility for mobile internet access; 6) Gateway GPRS Support Node (GGSN): It serves as interface between the mobile network and external networks like internet; 7) The Home Location Register (HLR) and Authentication Center(AuC): plays the same role as in 2G; 8) Operations Support System (OSS) : It provides and functionalities for monitoring and managing the 3G network. 
	   Indeed, the 3G is appreciated for enabling higher- speed services and covers different frequency bands depending on countries, ranging between 850Mhz - 1700 Mhz \cite{mishra2007advanced}.\\
	   
	   Additionally, the fourth generation, commonly referred to as LTE(Long-Term Evolution) represents a significant advancement over previous generations in terms of infrastructure and services. This generation introduces higher data speeds, improved capacity, and better perfomance for mobile communication and data services. The upgrades in infrastructure include: 1) BTS and MSCs : These components remain unchanged from the previous generation 2) Evolved Packet Core (EPC)  The EPC is a critical component of the 4G core network architecture which provides the packet-switched backbone that handles data traffic and ensures efficient data delivery between mobile devices and the internet or other networks; 3)  Radio Access Network (RAN): The Ran is responsible for the radio interface between mobile devices and base stations; 4) LTE (Long-Term Evolution) :  is the primary air interface enabling the high data speeds, low latency; 5) Back-haul Network : It connects base stations to the core network and internet infrastructure; 6) Spectrum Allocation: Hands over the mobile operator access to specific radio frequency bands; 7) Network Management System: These systems monitor and manage the 4G network, ensuring its smooth operation, performance optimization, and troubleshooting.
	   However, it's spanning or coverage of 4G networks on frequency bands allowed in each country based on their preferences, ranging from 700MHz to 2600 Mhz. The higher frequency bands generally offer faster data speeds but may have a shorter range, while the lower frequency bands can provide broader coverage but with slightly lower data speeds \cite{mishra2007advanced}.
	\subsection{Mobile Phone Models}  
	Since the mobiles phones are essential tools for communication, there is a wide range of popular mobile widely used by citizens of the DRC. The popular models come from various brands and offer a range of features to cater to different user preferences and needs. Some of the popular mobile phone models in DRC include \textit{ Techno, Itel, Infinix, Samsung, Apple, Huwaei, Itel, HTC, Motorola}. As it can be seen on the figure \ref{fig:mobilevendor}, according  to the  recent statistics conducted by \textit{Statistica},
	Samsung was the market leader in terms of share from January 2018 to November 2020, but in 2022, Tecno has emerged as the market leader.
	\begin{figure}
		\centering 
		\includegraphics[width=1\linewidth]{Images/mobileMarket.png}
		\caption{Market share of mobile device vendors in the Democratic Republic of the Congo from January 2018 to March 2022}
	    \captionsetup{position=top}
		\label{fig:mobilevendor} 
	\end{figure}
\\

Furthermore, all these models provided above sell the telephone following different types which include mobile phones, offering the features such as touchscreen displays, cameras, internet, connectivity, and access to mobile apps; features phones, which are basic mobile phones used for calling and texting; smartphones used by the majority (around 35\% in DRC ), providing access to mobile internet, mobile apps, multimedia messaging, and various productivity tools; Tablets, used for reading and for the same functionalities as smartphones.
\subsection{Mobile usage and prevalence} 
In fact, each phone has its own unique set of characteristics that define its capacity and performance compared to others. Some phones come with specific applications that can be used independently, even without being connected to an operator, such as a camera, calculator, games app, and many others. However, other phones may not have such features.\\

To access the services provided by the operator, the phone's sim-card must be functioning, recognized by the operator, and capable of sending and receiving communication signals. \textbf{Of course, all these services work only if the phone's battery has sufficient power}.\\

Moreover, the services that citizen's subscribers benefit from are as follows : 

Firstly, the Internet Access: The Internet services are used to connect people from different nodes. In fact, In accordance with the \textit{WorldBank} \footnote{WorldBank : International Telecommunication Union ( ITU ) World Telecommunication/ICT Indicators Database \url{https://data.worldbank.org/indicator}}  been used by 23\% of the DRC's population in 2020. Nonetheless, it requires payment which proportionally gives mobile data usually expressed in Megabytes. \\

Secondly, the Text Messaging (SMS): Even though the internet is the most used for texting, the SMS remains a widely used form of communication, especially in regions with \textbf{limited internet connectivity or among users who prefer simple text messaging or do not have the internet connection}.Furthermore, with the architecture of GSM(Global System for Mobile Communications) invented in the second generation, sending messages became possible. Nowadays, the web environment has developed application interfaces (API) that connect external systems to operators for sending messages \cite{hassinen2003secure}. One of the platforms that offer these services connects its SMS gateway to the GSM operator, as seen in the case of \textit{Octopush} \footnote{Octopush : SMS platform for businesses connected with their audience} architecture shown in Figure \ref{fig:smsgateway}  
\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{Images/SMSgateway}
	\caption{SMS Gateway Provider Architecture}
	\label{fig:smsgateway} 
\end{figure}
 \\
 
Thirdly, the Mobile Banking and Payments : With this services subscriber can make \textbf{financial transactions}; paying bills, transferring money conveniently. \\

Fourthly, the Mobile Entertainment: Mobile phones offer a range of entertainment options, from streaming videos and music to mobile gaming. 

And finally, the others Mobiles apps: This party includes the health services, education, social medias applications.
\section{Purposes of spammers in mobile messages}
Most of the time, spammers prefer to promise recipients prizes and then ask for money to claim the offer. They also attack SMS gateways with \textit{DoS} (Denial of Service) messages \cite{androulidakis2013fimess} whose goal is to overwhelm the system with unnecessary messages. Spammers send advertising and promotional messages based on company objectives, as well as SMS containing fake links or impersonating organizations to deceive recipients into taking certain actions or providing sensitive information \cite{tang2022clues}. Additionally, they may send SMS disguised as surveys to gather personal information for various fraudulent purposes. 
\section{Solutions}
To address theses problems, it is necessary to involve various stakeholders, including network operators, app developers, regulatory bodies, and users. \\

Firstly, it is recommended to implement mechanisms at the network level \cite{hao2009detecting} to filter messages and block users involved in spamming. \\

Secondly, users (subscribers) should be educated on how to analyze messages and report any one that is causing disturbances. \\

Thirdly, regulatory measures should be enforced to establish stringent regulations and penalties for spammers and those engaged in fraudulent mobile activities. Fourthly, the development of apps that enable filtering, classification, and reporting in the subscriber side would be beneficial.\\

 Fifthly, a website can be set to collect messages whether spam or ham reported by users who have doubts about their legitimacy, and then Machine Learning models can be used for detection purposes. For this, supervised or unsupervised methods can be employed to classify and predicting whether a message is spam or  ham. 
\section{Summary}
Overall, with the growth of mobile technologies, subscribers benefit from diversified services including : text messaging, voice calls, mobile baking, entertainment apps, and many others. However, These advancements also bring new challenges, such as the development of spam messages that aim to disturb network subscribers with unwanted or threatening messages. \\

In DRC, especially in the eastern party, users face similar issues. This chapter emphasizes methods or techniques that can be used to address this problem and relatively reduce spam. One of the prominent techniques suggested is based on Artificial Intelligence, particularly Machine Learning algorithms.
    
    \chapter{Review of the Literature and description of the approach}
    \section{Introduction}
    This chapter delves into theory, methodologies, and machine learning techniques, including relevant algorithms and their deployment in the suggested solution. It also highlights the contributions of previous researchers in the field.  
    \section{Revue of the Literature}  
    Numerous researchers have extensively explored the subject of spam detection. Within this domain, some have directed their investigations towards the web environment, while others have delved into realm of mobile	technologies.\\
    
    Furthermore, these researchers have chosen to investigate the detection of spam across various communication channels, including emails and SMS, encompassing Multimedia SMS (MSS) as well. In the following sections, we comprehensively review the body of work that has been accomplished within this context as follows: \\
         
	\cite{katankar2010short}. \textbf{Dr.V.M Veena K.Katankar}
	proposed a system that comprises an SMS gateway for transferring SMS messages after they have been stored and encrypted by the web server. This software operates through a web interface. Whenever a client sends a \textit{POST} request, it is received by the web server, which is responsible for encryption or decryption if necessary. Subsequently, the gateway transfers the message as per its designated route. This solution proves to be particularly valuable in mobile banking and organizational marketing systems. Nevertheless, the author encourages other researchers to delve into channel services in communication and advanced encryption techniques.\\
		
	\cite{brown2007sms} In their publication titled \textbf{\textit{Short Message Service}}, \textbf{Brown, Jeff and Shipman} members of IEEE, delve into several significant aspects. They start by exploring th growth of mobile phones and SMS services. They also examine the system architecture of SMS Centers and technologies used for message communication
	
	Furthermore, they shine the spotlight on aggregators and services providers. These are the entities that enable users to send bulk messages, essentially sending messages with a large amount of text to a group of recipients. This includes the interesting capability of converting email messages into SMS.
	
	Moreover, the article highlights that some of these aggregators may choose to collaborate with cellular networks. In this collaborative role, they act as intermediaries, connecting third-party entities that don't have direct relationships with cellular service providers. To achieve this, they employ a \textit{SMPP (Simple Messaging Peer to Peer)} protocols.\\
	
	\cite{medani2011review} \textbf{Researchers A. Medani and A. Gani}, affiliated with the University of Malaysia, have published a comprehensive review focusing on security concerns and techniques related to mobile Short Message Service (SMS).\\
	
	In their paper, they illuminate the process by wich a subscriber sends a message to another party while adhering to specific principles of the Over-The-Air (OTA) structure. This process involves transmitting the message from the sender to the base station and then forwarding it to the intended recipient trough the SMS Center (SMSC).\\
	
	Crucially, they emphasize the importance of securing every SMS using \textit{Public Key Infrastructure (PKI)}, ensuring end-to-end transmission security and safeguarding the message from unauthorized modifications. However, it's worth noting that the use of PKI can potentially impact mobile device performance due to the significant power requirements for the encryption process, and it may not guarantee integrity across all standards.\\
	
	To address these security concerns within GSM systems, the researchers propose the implementation of \textit{XML Key Management Specification} as a middleware solution. This middle ware system servers an intermediary, facilitating secure communication between mobile devices and enhancing overall system security for the benefit of clients.\\
	
	
	\cite{crawford2015survey} \textbf{Nikhil Kumar}, a reseacher affiliated with the University of New Delphi in India, published an article focusing on the topic of Email Spam Detection Using Machine Learning. In his study, he placed particular emphasis on comparing various machine algorithms, including \textbf{\textit{Naive Bayes, Support Vector Classifier, AdaBoosting, K-Nearest Neighbour, and Bagging Classifier}}. The objective was to predict wether an email was categorized as spam or legitimate (ham). To demonstrate his approach, he utilized an existing dataset available in the Kaggle workspace.\newline	
	Through experimentation and parameter tuning, Nikhil found that Naive Bayes delivered promising results in terms of accuracy. However, he also pointed out a limitation associated with the Naive Bayes algorithm. This limitation is tied to its assumption of class-conditional independence, which implies that each feature is considered independent of the presence of the other features. In cases where this assumption does not hold, it can lead to misclassification of data points.
	
	To address this limitation and enhance the performance of spam detection, the author recommended the use of \textbf{ensemble methods}. These methods involve the use of multiple classifiers for making class predictions, allowing for more robust and accurate results.\\
		
	\cite{navaney2018sms} In 2018, researchers Pavas Navaney, Gaurav Dubey, and Ajax Rana, who are affiliated with the University of Southern California and Amity presented a conference paper titled "SMS Spam Filtering using Supervised Machine Learning Algorithms".
	
	Their study concentrated on a dataset comprising 5574 records, of which 4827 messages were categorized as "ham" (legimate messages), while 747 messages were classified as "spam" (unsollicited or unwanted messages).
	
	The researchers applied three different machine learning methods to this dataset. Among these methods, it was observed that the \textit{Support Vector Machine (SVM)} algorithm achieved the highest accuracy compared to the Naive Bayes and Maximum Entropy Classifier algorithms. \\
	
	\cite{shirani2013sms}  \textbf{Houshmand Shirani-Mehr}, a researcher in Machine Learning, published an article in 2013 titled : "SMS Spam Detection using Machine Learning Approach". His purpose was to address the spam filtering problem by utilizing ML algorithms. Therefore, He utilized a dataset from the  \textit{UCI Machine Learning Repository} repository \footnote{\textbf{UCI ML} : The UCI Machine Learning Repository is a popular collection of datasets maintained by the University of California, Irvine (UCI). It serves as a valuable resource for researchers and practitioners in the field of machine learning and data mining. \url{https://archive.ics.uci.edu/}} , which contained real SMS messages. In development, he employed the algorithms to tackle that problem such as :  Naive Bayes with Laplace smoothing, Support Vector Machine, and Ensemble methods (\textit{AdaBoosting and Random Forest}). As an improvement, the author added meaningful features such as the length of messages in terms of the number of characters and certain thresholds.
	
	The results obtained after applying these methods to the dataset indicate that the SVM algorithm achieved the highest accuracy score.
	\\
	
	\cite{gupta2021sms} The authors of the article titled "SMS Spam Detection Using Machine Learning", namely
	\textbf{Gupta, Suparna Das and Saha, Soumyabrata and Das, Suman Kumar}. Their focus was on reviewing various techniques employed by other researchers in the realm of machine algorithms for SMS spam detection. \\
	
	In their research, they adopted a similar approach by incorporating the \textit{TF-IDF (Term Frenquency-Inverse Document Frequency)} method. This technique assesses the frequency of a word within a document and evaluates its importance in that document. \textit{TF-IDF} is a well-known method for measuring word relevance in a collection of texts.\\
	
	To assess the effectiveness of these techniques, the authors applied them to a spam dataset obtained from \textit{Kaggle} 
	\footnote{ \textbf{Kaggle :} a platform for data science competitions and datasets. \url{https://www.kaggle.com/}}
	. After conducting their experiments and evaluations, the authors arrived at a noteworthy conclusion. They found that among all the ML algorithms they employed, the Naive Bayes algorithm consistently achieved the highest level of accuracy in SMS spam detection.\\
	
	As shown above, many researchers have investigated the same topic using different approaches. Some have focused on security within mobile architecture, including message transfer processes, while others have concentrated on using Machine Learning (ML) models to combat the issue of spam. In general, these approaches are valuable to this project and serve as its inspiration at the point that many techniques related to these approaches have been implemented in this project.\\

    However, what sets this project apart is its pratical approach involving specific society. Rather than solely relying on existing datasets from platforms like \textit{Kaggle and UC Machine Learning Repository}, this project has actively engaged with people to collect data. It has also integrated some data from these platform datasets to enhance the quality of information. \\
        
    Furthermore, this project harnesses the latest advancements in machine learning. It utilizes Ensemble Methods to achieve high levels of accuracy. Addionnally, it employs technical parameter tuning, including \textit{GridSearcher and VotingClassifier}. In fact, GridSearcher assists in identifying the most suitable parameters required for algorithm models. \\
    
    Moreover, this project doesn't stop at model development, it extends to the deployment of the models generated through the processes. It provides backend \textit{APIs} for certain platforms interested in learning from these results. \\
    
    Additionally, it outlines the structure of GSM deployment, encompassing the SMSCenter's role in the mobile messaging system.
   
   
    
   \section{Tools and Techniques} 
	In the mission of this project to create a messaging application that not only streamlines mobile communication but also tackles the pervasive issue of spam, this section serves as a technical guide. It will explore the tools and techniques at the core of the approach used. \\
		
	Building an effective messaging application is like constructing a house- you need the right tools. Thus, this section discusses the software and technologies that form the foundation of our messaging app, including the programming languages, frameworks, and platforms utilized. \\
	
	To combat spam effectively, this project is enlisting the help of machine learning. It delves into the world of data analysis and machine learning tools and frameworks : \textit{numpy, pandas, matplotlib, scikit-learn,} that empower the authors to analyze, detect and prevent spam messages.\\
	
	\subsection{Messaging application development tools}
	In the realm of modern software development, the choices of development tools can significantly impact the efficiency and effectiveness of the project. When crafting application, the authors carefully considered the tools that will shape the foundation of the \textit{backend} and \textit{frontend} development. Actually, the \textit{backend} references the environment where data of the app are stored, structured and more secured; while the \textit{frontend} involves the space where techniques are developed to show to the user the interface comfortable for his understanding. Among all tools, some serve as programming languages and others as editors. \\ 
	
	Hence, The choices made by the authors are \textit{Python (with Django \textit{Framework})} as programming language and \textit{SQL} for data structuring language in \textit{backend} development and HTML, CSS and Javascript (with \textit{Vue-Js Framework}) for \textit{frontend} environment. \\
	
	\subsubsection{Back-end Development with \textit{Python} (\textit{Django}) and \textit{SQL} :}  
	\includegraphics[width=0.2\linewidth]{pythonImage.png}
	 is a powerful language appeared in 1980 firstly implemented by Guido van Rossum at \textit{Stichting Mathematisch Centrum} in the Netherlands as a successor of a language called  \textit{ABC} \cite{tulchak2016history}. It newest version is \textit{Python 3} \footnote{Python : \url{https://www.python.org/}}  which is available for all most environments, either \textit{Macos} or \textit{Linux} and \textit{Windows}.\\
	 	 
	 In comparison with {Java, C, C++, MATLAB}, \textit{Python} is more readable, since it requires few lines of code which are clean. Next, it is a good choice for those who want to start with programming \cite{bogdanchikov2013python}. Technically it is versatile, since it used for a wide range of applications, from web development to mobile apps (with Kivy \footnote{Kivy: \url{https://realpython.com/mobile-app-kivy-python/}} ), data analysis, machine learning, and scientific computing. Actually, the high reason that influenced the use of this programming language is its capacity to deal with data by proving scientists with many libraries. \\
	 
	 Since the frameworks help to gain time in terms of development and structuring a project, \textit{Python} provides many frameworks for web development. Notable these are Django, Flask, and more. Authors's choice, Django, is a high-level, open-source web framework for building robust and dynamic web applications. It's written in \textit{Python}, which is one of the reasons for its popularity. It follows the \textbf{\textit{batteries-included}} philosophy, providing a wealth of built-in features like authentification, URL routing, a powerful admin interface, and an ORM (Object-Relational Mapping) system, which simplifies database operations \cite{alchin2013pro}. \\
	 
	Actually, Django's ORM abstracts many \textit{SQL} complexities, enabling developers to interact with the database using \textit{Python} code, without needing to write raw \textit{SQL} queries. This higher-level interaction simplifies database access and makes the development process more efficient. So, while \textit{SQL} is at core of database interaction, Django's \textit{ORM} providers a user-interface for developers, streamlining the development of data-driven web applications.\\ 
	
	Suppose we want to retrieve all the employees in a database with salaries greater than \$4000 using raw SQL. Here's what the SQL query would look like : 	
\begin{lstlisting}[style=htmlcssjsstyle]
 SELECT * FROM employees WHERE salary > 4000
\end{lstlisting}
  While Django accomplishes the same task using the following code:
\begin{lstlisting}[style=stylejupyter]
from myapp.models import Employee
  employees = Employee.objects.filters(salary__gt = 4000)
\end{lstlisting}
    \subsubsection*{How did we get there ?}
   In fact, the project must be running for doing that. For installing \textit{Python}, just go to the official website for the  download, no matter the OS version, either \textit{ \href{https://www.python.org/downloads/macos/}{MACOS} , \href{https://www.python.org/downloads/windows/}{Windows} } or \href{https://www.python.org/downloads/source/}{\textit{Linux/UNIX}} since \textit{Python} is portable. Indeed, the last version at the writing of this project was Python 3.12.0. To test if the it is working, just enter the command \textcolor{blue}{python} as in figure \ref{fig:testpython}.
   \begin{figure}
   	\centering
   	\includegraphics[width=1\linewidth]{testPython}
   	\caption{Testing if python is running on the OS (LINUX)}
   	\label{fig:testpython}
   \end{figure}
   
   
   Next, for installing Django, the steps remain pretty the same, going on the official page and following the guides as resumed in the as follows:   
   For the step 1, Installing the virtual environment ( \textit{V.M}) \footnote{How to create the VM ?https://realpython.com/python-virtual-environments-a-primer/}    : \\
   
   \textbf{In Windows} : \texttt{python -m venv myenv}; then active it by : \texttt{myenv\textbackslash Scripts\textbackslash activate} \newline
   The word \textit{myenv} is just the name of virtual environment. But Why do we create it ? In fact, it is a best practice in Python development to manage dependencies, isolate projects, and maintain a clean and organized development environment. \\
    
  In \textbf{MACOS/Linux} : Only the way of activating the \textit{VM} changes. Then just by entering the following command:\newline
  \texttt{source myenv/bin/activate}; the virtual will be functioning. \\
  
   \textbf{Finally}, as the \textit{VM} is working Django, can be installed, by the command :  \texttt{pip install Django}. At the writing of this project, the last version 4.2.6. The bit steps to follow for installing are demonstrated in the figure \ref{fig:installdjango}
\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{installDjango}
	\caption{How to install django ? Here, the name of \textit{VE} is SpamAppEnv}
	\label{fig:installdjango}
\end{figure}
\\
 Now, the project can be created, followed by the app inside, depending on interests. For that the commands  : \textcolor{blue}{ \texttt{django-admin startproject projectname}} and : \textcolor{blue}{\texttt{django-admin startproject appname}} can be utilized. The result is demonstrated in figure \ref{fig:visualstudio}. 
\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{visualStudio}
	\caption{After the project and app are already created}
	\label{fig:visualstudio}
\end{figure} 
 \\
 
  However, the configuration about the app created, ought to be made lest it should raise the error. Thus, in settings file, present in the project folder the properties called \textit{INSTALLED\_APPS}, as in the figure \ref{fig:appinstallname}.\newline After, this we can just enter the commands : 
 \begin{lstlisting}[style=stylejupyter]
 python manage.py makemigrations
 python manage.py migrate
\end{lstlisting}
For sure, the first command line says to Django the new changes, and the second applies or assesses them. 
\begin{figure}
	\centering
	\includegraphics[width=1\linewidth, height=3.5cm]{appInstallName}
	\caption{Adding the appname in \textit{INSTALLED\_APPS} dependencies }
	\label{fig:appinstallname}
\end{figure} 

\subsubsection*{How does Django deal with the database ?} 
Django deals with databases through \textit{ORM} as mentioned above. Its management involves to follow different steps such as:

\textbf{Database Configuration } : in the Django project's settings (usually in the \textit{settings.py}, we have to specify the database we want to use. Django supports various databases types, including \textit{PostgreSQL, Mysql, SQLite and Oracle} \footnote{Setting Databases in Django : \url{https://docs.djangoproject.com/fr/4.2/ref/databases/\#mysql-notes}}  . We have only define the database backend, connection details, and other options in the \textit{DATABASES} setting. For instance the configuration of \textit{MYSQL} will be like this : 
\begin{lstlisting}[style=stylejupyter]
DATABASES = {
  'default': {
  'ENGINE': 'django.db.backends.mysql',
  'NAME': 'the_db_name',
  'USER': 'the_db_user',
  'PASSWORD': 'the_db_password',
  'HOST': 'localhost',  # or the IP address of the MySQL server
  'PORT': '3306',  # MySQL default port
 }
}
\end{lstlisting} 

\textbf{Model Definitions} : \textit{Django} models are Python classes that define the structure of the database. Models are defined in the the \textbf{\textit{models.py}} file. Each model class represents a database table, and each model field represents a table column. The syntax used for creating a database is as follows: 
\begin{lstlisting}[style=stylejupyter]
from django.db import models
class Employee(models.Model):
     name = models.CharField(max_length=100)
	     salary = models.DecimalField(max_digits=10, decimal_places=2)
\end{lstlisting} 
The corresponding \textit{SQL} code is this:  
\begin{lstlisting}[style=htmlcssjsstyle]
	CREATE TABLE Employee (
	 id INTEGER PRIMARY KEY,
	 name VARCHAR(100),
	 salary NUMERIC(10, 2)
	);
\end{lstlisting}  

 \textbf{Migrations} : Every time a model (database table) is created or modified, \textit{Django} need to notified for assessing these changes. As mentioned earlier, the command'\textbf{python manage.py makemigrations}' is executed in the terminal for managing the changes,  '\textbf{manage.py migrate}' command is entered to apply all of these changes.   \\
 
\textbf{Database abstraction} : With this technique, we do'nt no longer need to write \textit{SQL} code, since by \textit{Python} it is possible to interact with the database, and making a \textit{queryset} request(\textit{crud}  \footnote{CRUD: Create Read Update Delete item}).Let's see how it works once again: 

\begin{lstlisting}[style=stylejupyter]
new_employee = Employee(name ="Eistein", salary=45OOOO) 
new_employee.save()
\end{lstlisting} 
instead of doing this in SQL : 
\begin{lstlisting}[style=htmlcssjsstyle]
INSERT INTO employees (name, salary) VALUES ('Einstein', 45000);
\end{lstlisting}  

Indeed, we can see that even a non-professional in \textit{SQL} can now deal with the database without any \textit{SQL} code. \newline
Furthermore, the use of abstraction techniques to interact with database, enhances its security of by guarding against \textit{SQL injection}. 
\subsubsection{How does Django do for interacting with APIs ?}

\textit{Django} presents a useful package for interacting with the \textit{API} called \textit{DRF \footnote{DRF (Django Rest Framwork) : a powerful tools serving to interact with Apis. \url{https://www.django-rest-framework.org/}}} It allows : authentication policies including optional packages for \textit{OAuth1a} and \textit{OAuth2 \footnote{OAuth2 :   (Open Authorization 2.0) : is a framework that allows third-party applications to access a user's data without exposing their credentials, such as passwords.OAuth 1.Oa is the old version of OAuth2  }}, web browsable API  and serialization \footnote{Serialization : converting data into formats like \textit{JSON}, \textit{XML}, etc. The deserialization involves the reconstruction of the same operation} and deserialization that supports both \textit{ORM} And \textit{NO-ORM} data sources \cite{nader2023django}. 
 
\subsubsection{\large \textit{Front-end} Development with \textit{HTML}, \textit{CSS} and \textit{Js} :} 
The Front-end development is a crucial aspect of web development that focuses on creating the user interface and user experience of a website or web application. It involves using a combination of HTML,CSS, and Javascript to build the visible and interactive elements of a website. \\

The HTML (Hypertext Markup Language) is used as a maker of web pages by providing its structure and content. It uses a markup language with various tags to define headings, paragraphs, links, images, forms, and more \cite{stark2010building}. For example a page with a heading and a paragraph should look like this in the content :
\begin{lstlisting}[style=htmlcssjsstyle]
 <div>
     <h1>Page Dev</h1>
     <p>this is our page</p>
 </div}
\end{lstlisting}

Furthermore, the \textit{CSS (Cascading Style Sheets)} is used for styling and layout. Thus, it controls the visual presentation of HTML elements. For example for our above code : \newline
\begin{lstlisting}[style=htmlcssjsstyle]
h1{
   color : blue; /* set the color to the element*/
   font-size: 24px; /* sets the font size */
   text-align: center; /* centers the element*/
}
\end{lstlisting} 

Apart from that, the other powerful tool used in web development is \textit{JS(JavaScript)}. It adds interactivity and dynamic behavior to web pages. It also leveraged for creating features like images sliders, form validation, animations and real-time updates without having to \textbf{reload the page}. The common frameworks used to extend its productivity are : React, Angular and Vue-js \cite{wohlgethan2018supportingweb}. By updating the above \textit{HTML} code, we can see the interactivity created by JS: 
\begin{lstlisting}[style=htmlcssjsstyle, label=lst:jsCode]
<!DOCTYPE html>
<html>
<head>
 <title>JavaScript Example</title>
</head>
<body>
 <div>
 <h1 id="pageTitle">Page Dev</h1>
 <p>this is our page</p>
 </div>
	
 <button id="changeTitleButton"  onclick="changeTitle()">Change  Title</button>
	
 <script>
 	// JavaScript function to  change the title
 	function changeTitle() {
 	// Get the h1 element by its ID
 	var titleElement =  document.getElementById("pageTitle");
	
 	// Check if the element exists
 	if (titleElement) {
	// Change the text of the h1 element
 	titleElement.innerText = "New Page Title";
	  }
	}
 </script>
</body>
</html>	
\end{lstlisting} 

In this example, the button with the ID 'ChangeTitleButton' and the function called 'ChangeTitle()' are added. The click on the 'Change Title' button executes the changeTitle function. Then, the function retrieves the \texttt{<h1>} element by its
ID ("pageTitle") and changes its text to "New page Title". Thus, by this bit snippet code, \textit{JavaScript} really appears interactive.

\subsection{\large Machine Learning tools and frameworks} 
Machine learning involves learning from data, visualizing, analyzing it, and making predictions. To do this, we need the right tools. \textit{Python} is one of best and powerful tools used for these tasks. It has a large and active community that continuously contributes to its growth. \textit{Python} offers a variety of packages that assist data scientists in their work.\\

In Python, common packages used for working with data include\textbf{ numpy, pandas, matplotlib, seaborn, scikit-learn, tensorflow}, and more.

The Integrated Development Environment (\textit{IDE}) used for working in Python, even for all backend-development is : Visual studio. It is portable and downloadable from the official page( \url{https://code.visualstudio.com/}). Additionally, we have another \textit{IDE} called \textit{Anaconda} including \textit{Jupyter notebook} which is used for machine learning and data science projects. It's designed to simplify package management and deployment by using a package manager called \textit{Conda} which helps users to install, update, and manage packages, libraries and dependencies \cite{teimourzadeh2022application}.\\

To start a new project we just execute in the terminal, the command :  \texttt{jupyter notebook}, then a default browser configured just open directly the link. The page look like in the fig \ref{fig:jupyteropen}.
\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{jupyterOpen}
	\caption{Start jupyter in conda kernel}
	\label{fig:jupyteropen}
\end{figure}  

\subsubsection{{\large Data manipulation with \textit{Numpy}, \textit{Pandas}}}
  {\textbf{Numpy}} Library\\
 
   Numpy is a fundamental library for scientific computing with Python. It provides support for large, mutli-dimensional arrays and matrices, along with an assortment of high-level mathematical functions to operate on these arrays.\\
   
 Actually, the reason of thinking about a new tool of computing in Python is because, all  data structures it provides: lists for enumerating a collection of objects, dictionaries to build hast tables, are not ideally suited to high-performance numerical computation \cite{van2011numpy}.\newline 
 Basically, the usage of numpy asks for importing its modules like this :  
 \begin{lstlisting}[style=stylejupyter]
 import numpy as np  #np is a common alias used  
 In [4]: np.__version__
 Out [4] : '1.24.3'
 \end{lstlisting} 
 %% We assume that the np alias created will be used for other demonstration.\newline
 \textbf{The structure and creation of a \textit{NumPy} array}. The fundamental data structure provided by the Numpy library for representing arrays is \textbf{ndarray} it refers to the array which is n-dimensional or multi-dimensional \cite{coursAnalyseDonne2}. Thus, several means can be used to create an array as follows  : \newline
 - With 1D(dimension):
 \begin{lstlisting}[style=stylejupyter]
In[8] : np.array([1, 2, 3, 4, 5])
#Creates a NumPy array from a given list or iterable.
Out[8] : array([1, 2, 3, 4, 5]) 

#2D array
In[12] : np.array([[1, 2, 3], [4, 5, 6]])

In[12]: array([[1, 2, 3],
		[4, 5, 6]])
 \end{lstlisting}
 \begin{lstlisting}[style=stylejupyter]
In[10] : np.arange(1, 10, 2)  
# Creates a 1D array from 1 to 9 with a step of 2
Out[10] : array([1, 3, 5, 7, 9]) 
#2D array for arange function, requires manipulation
 \end{lstlisting} 
  Many other functions can be used for creating arrays, like : empty, zeros, ones, eye, etc. \\
  
 \noindent\textbf{Manipulation of arrays}. The manipulation operation includes : The splitting, slicing, indexing,reshaping, arithmetical operations, concatenations, comparisons and many others. Let's dive into some of that operations as follows:
 \begin{lstlisting}[style=stylejupyter] 
 #We generate the integers from zero to twelve and
 # re pack them into a 4x3 array
 In [21] : np.arange(12).reshape((4,3))  
 Out [21] : array([[ 0,  1,  2],
 		    [ 3,  4,  5],
 		    [ 6,  7,  8],
 		    [ 9, 10, 11]]) 
 #Suppose we want to multiply each vector element by 3
 In [24] :  a = [3,7,4] 
           [4*x for x in a]
 Out[25] : [12, 28, 16]  
 #Concatenate arrays vertically  
 In [29] : np.vstack(([1, 2, 3],[1, 2, 3]))  
 Out [29] : array([[1, 2, 3],
                   [1, 2, 3]]) 
 #Concatenate arrays horizontally
 In [31] : np.hstack(([1, 2, 3],[1, 2, 3]))
 Out [31] : array([1, 2, 3, 1, 2, 3])   
 
 
 \end{lstlisting} 
 Let's mention that ndarray object is homogeneous since all elements within must have the same data type. The types allowed are : Float, int, bytes, str, number and complex (for decimal complex numbers). To define, the type on the array on creation is made by \textbf{dtype} property like this: 
\begin{lstlisting}[style=stylejupyter] 
 In [55] : np.array([1, 2, 3, 4, 5], dtype='float') 
 Out [55] : array([1., 2., 3., 4., 5.]) 
\end{lstlisting} 

Besides, dealing with slicing and splitting still depends on the dimension. Lets break in the code to see that : 
\begin{lstlisting}[style=stylejupyter]
In [56] : arr = np.array([[1, 2, 3],
[4, 5, 6],
[7, 8, 9]]) 

#split along rows (axis=0)
In [59] : split_rows = np.vsplit(arr, 3) # Split into threee 1-row arrays
Out[59] : [array([[1, 2, 3]]), array([[4, 5, 6]]), array([[7, 8, 9]])]

#slicing 
In [60] : arr[1:3, 1:3]  # Get a 2x2 subarray
Out [60] : array([[5, 6],
		  [8, 9]])
\end{lstlisting}   
 Some others functions are used as follows : all, any, cov-corrcoef, dot, where, random(), randint() and many others. The details of usage are given on the official of Numpy package \footnote{Numpy, official page : https://numpy.org/}.
\\

\subsubsection*{{\large  Pandas library :}} 
\textit{Pandas \footnote{https://pandas.pydata.org/} } is an essential open-source Python library used for data manipulation and analysis. It provides functions for reading and writing data structures in various formats, including \textit{CSV} and text files, Microsoft Excel, SQL databases, and the fast \textit{HDF5} \footnote{HDFR (Hierchical Data Format version 5) : It is a versatile and high-performance data storage format commonly used in scientific and data analysis fields.} format. Additionally, Pandas can \textbf{align data, handle missing data, allow reshaping and pivoting, perform data aggregation and transformation, and merge and join data sets} \cite{mckinney2015pandas}. \newline It offers many other capabilities for efficient data processing. 
To start with \textit{pandas}, we have to create the pandas  object like this: 
\begin{lstlisting}[style=stylejupyter]
import pandas as pd # pd is an alias name
\end{lstlisting}

\textbf{Data Structures}.
Pandas has two main structures: Series and DataFrame. The first is one-dimensional array, while the second is a two-dimensional table that is similar to a spreadsheet or SQL. Let's break into an example: 

- Series:
\begin{lstlisting}[style=stylejupyter]
# Creating a Series from a list
In [46] : data = [1, 2, 3, 4, 5]
Out [46] : pd.Series(data) 
0    1
1    2
2    3
3    4
4    5
dtype: int64
\end{lstlisting} 
- DataFrame: \newline 

\noindent \includegraphics[width=16cm, height=3.7cm]{demoDataFrame}
\\

\textbf{Reading and Writing data.} Pandas offers versatile functionality which deal with various sources and formats. The common sources include : \newline
\textbf{- CSV files}: They are read and written like this : 
\begin{lstlisting}[style=stylejupyter]
 #Reading from CSV
 data = pd.read_csv('dafileName.csv') 
 #Writing to CSV
 data.to_csv('new_data_file_name.csv', index=False)
\end{lstlisting} 

\noindent \textbf{- Excel files} : 
 \begin{lstlisting}[style=stylejupyter]
# Reading from Excel
 data = pd.read_excel('dafileName.xlsx', sheet_name='Sheet1')

# Writing to Excel
 data.to_excel('new_data_file_name.xlsx', sheet_name='Sheet1', index=False)
 \end{lstlisting} 
 
 \textbf{- SQL Databases} :
  \begin{lstlisting}[style=stylejupyter]
# Reading from SQL database sqlite
connection = sqlite3.connect('my_database.db')
query = 'SELECT * FROM my_table'
data = pd.read_sql(query, connection)

# Writing to SQL database
data.to_sql('new_table', connection, if_exists='replace', index=False)
 \end{lstlisting}
 
In the same way, functions \textbf{read\_json, read\_html}  are also respectively used for dealing with the JSON and HTML data.\\

\textbf{- Data Cleaning.} Pandas uses several functions, among them we have drop\_duplicates(), fillna(). Which are used like this:
\begin{lstlisting}[style=stylejupyter]
# Removing duplicates
df = df.drop_duplicates()

# Handling missing values
df = df.fillna(0)
\end{lstlisting} 

\textbf{- Data Exploration}. Pandas provides methods for exploring the dataset, such as \textbf{head(), tail(), describe(), and info()}. Thus, the application will look like :
\begin{lstlisting}[style=stylejupyter]
data = {'A': [1, 2, 3, 4, 5],
'B': ['X', 'Y', 'Z', 'X', 'Y']}
df = pd.DataFrame(data)
# Display the summary statistics of numeric columns
print(df.describe()) 

              A
count  5.000000
mean   3.000000
std    1.581139
min    1.000000
25%    2.000000
50%    3.000000
75%    4.000000
max    5.000000
\end{lstlisting} 

\noindent \textbf{Data Manipulation}. Pandas performs the manipulation of tables as Excel using the pivot\_table() and merge() function perform combination operation for analysis.
Additionally it allows indexing, slicing, filtering, modifying data. Let's break into the demonstration:
\newline
\textbf{- Pivot table }:
\begin{lstlisting}[style=stylejupyter]
In [72] : df.pivot_table(index='B', values='A', aggfunc='mean')
print(pivot) 
     A
B     
X  2.5
Y  3.5
Z  3.0 

#The operation computed is the average measurement
\end{lstlisting} 

\textbf{- Merging} :
\begin{lstlisting}[style=stylejupyter]
 #Create two DataFrames
 data1 = {'Key': [1, 2, 3, 4, 5],
'Value1': ['A', 'B', 'C', 'D', 'E']}
data2 = {'Key': [3, 4, 5, 6, 7],
'Value2': ['X', 'Y', 'Z', 'U', 'V']}
df1 = pd.DataFrame(data1)
df2 = pd.DataFrame(data2)

# Perform combination-like operation
result = pd.merge(df1, df2, on='Key', how='left')
print(result) 

Key Value1 Value2
0    1      A    NaN
1    2      B    NaN
2    3      C      X
3    4      D      Y
4    5      E      Z
\end{lstlisting} 

\textbf{- Slicing}:
\begin{lstlisting}[style=stylejupyter] 
#Consider the example on the top about 
#name, age, city
# Slice specific rows and columns
  df.loc[1:3, 'Name':'Age']  
#From the second column till the threeth line 
#From the Name till the Age column
#The result gives this: 

Name  Age
1      Bob   30
2  Charlie   35
\end{lstlisting} 
Assume that we want to apply some characteristic on a column :
\begin{lstlisting}[style=stylejupyter]
# Create a new column based on an existing column
df['days'] = df['Age'].apply(lambda x: x * 360)
print(df)
# The result is like this: 
      Name   Age    City       days
0    Alice   25     New York   9000
1      Bob   30  Los Angeles  10800
2  Charlie   35      Chicago  12600
\end{lstlisting}

\textbf{- Modifying Data} : 
\begin{lstlisting}[style=stylejupyter]
df.loc[2, 'Name'] = "Jonathan"
print(df)
#The df becomes like this : 
Name  Age         City   days
0     Alice   25     New York   9000
1       Bob   30  Los Angeles  10800
2  Jonathan   35      Chicago  12600
\end{lstlisting} 

\subsubsection{Data Visualization With \textit{Matplotlib} and \textit{Seaborn}} 
\subsubsection*{\textit{Matplotlib}}
\textit{Matplotlib} is a open-source, flexible and customizable Python package used for creating 2D and 3D \footnote{mpl\_toolkits.mplots3d : \url{https://matplotlib.org/stable/tutorials/toolkits/mplot3d.html}, tools used for generating 3D plots } plotting having a high production-quality. In addition to this, it includes the capacity of saving images in different output formats (JPG, PNG, PS and others) \cite{tosi2009matplotlib}. 

To get started with it, we just go to the official page and download the dependencies \footnote{Matplotlib: \url{https://matplotlib.org/}}. However, for who are those using integrated development environment (\textit{IDE}) like \textit{Anaconda} \textit{numpy}, \textit{pandas}, and \textit{matplotlib} are incorporated inside.
When working with data visualization, the type of plot to choose should depend on the nature of the data. In the table \ref{tab:plotTypeDataTable} , the overview of types of plots and when to use them is given. However, some plots can be combined even though they are primarily made for whether categorical or numerical visualization. 

\noindent The usage of \textit{matplotlib} properties require a good manipulation of data, often done by the \textit{numpy} and \textit{pandas}. Let's break into examples of its usage: \newline
First all, the import of \textit{matplotlib} is required for any manipulation. It is done like this :
\begin{lstlisting}[style=stylejupyter]
import matplotlib.pyplot as plt #plt is also an alias
\end{lstlisting} 
 
\begin{lstlisting}[style=stylejupyter]
import matplotlib.pyplot as plt
import numpy as np

# Data
x = np.arange(1, 11)
y1 = x
y2 = x**2
y3 = np.sqrt(x)

# Create a figure with subplots (1row and 3 columns)
fig, axs = plt.subplots(1, 3, figsize=(15, 5))

# First Subplot: Line Plot   
axs[0].plot(x, y1, color='b', marker='o') # 'o' is a clue

#define legends
axs[0].set_title('Line Plot')
axs[0].set_xlabel('X-axis')
axs[0].set_ylabel('Y-axis')

# Second Subplot: Bar Plot
axs[1].bar(x, y2, color='g', alpha=0.6)
axs[1].set_title('Bar Plot')
axs[1].set_xlabel('X-axis')
axs[1].set_ylabel('Y-axis')

# Third Subplot: Scatter Plot 
axs[2].scatter(x, y3, color='r', marker='x')
axs[2].set_title('Scatter Plot')
axs[2].set_xlabel('X-axis')
axs[2].set_ylabel('Y-axis')

# Adjust subplot layout to prevent overlapping
plt.tight_layout()

# Display the figure
plt.show()
\end{lstlisting}  

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{matplotLibExample}
	\caption{How to use \textit{matplotlib} for visualization}
	\label{fig:matplotlibexample}
\end{figure} 

Additionally, \textit{matplotlib} is used for creating 3D as follows : 
\begin{lstlisting}[style=stylejupyter]
	from mpl_toolkits.mplot3d import Axes3D
	import matplotlib.pyplot as plt
	import numpy as np
	
	fig = plt.figure()
	ax = fig.add_subplot(111, projection='3d')
	
	x = np.random.rand(10)
	y = np.random.rand(10)
	z = np.random.rand(10)
	
	ax.scatter(x, y, z, c='r', marker='o')
	
	ax.set_xlabel('X-axis')
	ax.set_ylabel('Y-axis')
	ax.set_zlabel('Z-axis')
	
	plt.show()
\end{lstlisting} 
Notice that: \textit{matplotlib}, uses \textit{mpl\_toolkits} for integrating with 3D, since at its release it was creating 2D plots only. 
Besides, the difference observed in the code at axis where the function \textit{add\_subplot} receives 111 value as the first argument. It just defines that the plot will be a single for all 3 dimensions as shown in the figure \ref{fig:3ddimensionPlotlib}. 
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{3Ddimension}
	\caption{3D plot with Matplotlib}
	\label{fig:3ddimensionPlotlib}
\end{figure}

\begin{table}
	 % Color every other row
\begin{tabular}{|p{3cm}|p{3.5cm}|p{8cm}|} 
\hline
\hline 	
 Type of data	& Plot name & Utilities  \\

\hline
\hline
\centering
\multirow{10}{3cm}{{\large Categorical Data}} &   Bar Plot&  It is deal for showing the frequency or count of categorical data. \newline
Example: Comparing the number of apples, bananas, and oranges sold at a fruit stand \\ 
\cline{2-3} 

 & Pie Chart & Suitable for displaying parts of a whole. \newline
Example: Showing the composition of monthly expenses (eg. rent, groceries, utilities).\\ 
\cline{2-3}
 
& Stacked Bar Chart & Useful for comparing categories while also showing their composition\\
\hline
\centering
\multirow{14}{3cm}{\large Numerical Data} & Histogram & Visualizes the distribution of a single variable or continuous data.\newline 
Example : Analyzing the distribution of ages in a population\\
\cline{2-3}

& Box Plot & Displays the distribution and spread of numerical data\\
\cline{2-3}

 & Scatter Plot & Depicts relationships between two numerical variables \newline Example : Showing the correlation between the number of study hours and exam scores for multiple students.
 \\
\cline{2-3}

 & Line Plot & Shows changes in variable over a continuous range, often over time. \newline Example : Plotting stock prices over several months to visualize trends. \\
 \hline
\centering

\multirow{8}{3cm}{{\large Mixed Data}} & Violion Plot & Combines a box plot with a kernel density estimation to visualize the distribution. \newline Example: Comparing the distribution of test scores across different schools.\\
\cline{2-3}
& Heatmap & Useful for displaying relationship between multiple variables in matrix. \newline Example : Analyzing the correlation between various factors (eg., age, income, and education level) in a survey.\\
\hline 

\end{tabular}    
\caption{Choose the plot depending on data's nature}
\label{tab:plotTypeDataTable}
\end{table}

\subsubsection*{\textit{Seaborn}}
\textit{Seaborn} is another Python library for statistical data visualization, built on \textit{matplotlib}. It just provides properties able to create more informative an visually appealing graphics \cite{sial2021comparative}. To get started with it, we just visit the official page \footnote{Seaborn: \url{https://seaborn.pydata.org/}} which presents all steps required for installing. Alternatively, we can use an  the integrated development environment \textit{IDE} like \textit{Anaconda} once again, which already includes it. 

The following example shows how to combine both \textit{matplotlib} and \textit{seaborn}:  
\begin{lstlisting}[style=stylejupyter]
import matplotlib.pyplot as plt 
import seaborn as sns # Its how to start the 

# Create a Seaborn plot
sns.set(style="whitegrid")
tips = sns.load_dataset("tips")
ax = sns.barplot(x="day", y="total_bill", data=tips)

# Customize the plot using Matplotlib
ax.set(xlabel="Day of the Week", ylabel="Total Bill Amount")
plt.title("Average Total Bill Amount by Day")

#Save the plot as an image
plt.savefig("seaborn_customized_plot.png")
plt.show() # Show the plot
\end{lstlisting} 
The data used in the following code, named 'tips,' is provided for practice purposes. \textit{Seaborn} is used to generate the plot, while \textit{matplotlib} customizes it by adding a legend and saving the image generated in figure.


\subsubsection*{\large Machine learning with scikit-learn} 
Since Python programming language is establishing itself as one the most popular languages for scientific computing. Many and various libraries are developed inside making it more and more appealing \cite{pedregosa2011scikit}. \\

\textit{Scikit-learn} is an open-source and commercially(usage - BSD license) machine learning library for the Python programming language. It was initially developed by David Counapeu in 2007 as part of the Google Summer of Code project. Since then, it has grown to become one the most popular and widely used Machine Learning libraries in Python ecosystem \cite{scikitlearnCode1}.   
The techniques and properties used inside of it, allows it to predictive data analysis by performing the  classification, regression, clustering, preprocessing and more other tasks.\\

To get started with it as usually, the official page present all the steps for downloading all the packages or using \textit{IDE} like \textit{Anaconda} which encompasses it. \newline
Before using its functions we have to initialize the by import the class which includes the methods and properties achieving a given task. For example, suppose that we are about to process data a class called \textit{StandardScaler} can be useful \textbf{when the dataset contains features with different scales} . Then, it standardizes the data, centering it around 0 and ensuring that it has a standard deviation close to 1.
\begin{lstlisting}[style=stylejupyter]
In[2]:  from sklearn.preprocessing import StandardScaler
	import numpy as np
	
	# Example data
	data = np.array([[1.0, 2.0, 3.0],
	[4.0, 5.0, 6.0],
	[7.0, 8.0, 9.0]])
	
	# Create a StandardScaler instance
	scaler = StandardScaler()
	
	# Fit the scaler to the data and transform the data
	data_standardized = scaler.fit_transform(data)
	
	print("Original Data:")
	print(data)
	print("Standardized Data:")
	print(data_standardized)  
	
Out[2]:
	
	Original Data:
	[[1. 2. 3.]
	[4. 5. 6.]
	[7. 8. 9.]]
	Standardized Data:
	[[-1.22474487 -1.22474487 -1.22474487]
	[ 0.          0.          0.        ]
	[ 1.22474487  1.22474487  1.22474487]]
	
\end{lstlisting}
  We just notice that, this time, all the standardized data have a mean close to 0 and a standard deviation close to 1. For that, the data become centered around zero and have now consistent units of measurement. \\
  
  Additionally, the \textit{\textbf{fitstransform()}} method on the object, defines both fitting and transformation process. Actually, the \textit{\textbf{fit()}} method computes the mean and standard deviation of the trained data and \textit{\textbf{transform()}} method scales the trained data based on the mean and standard deviation. Indeed,  \textit{fitstransform()} includes all both. \newline Actually, the explanations about training, fitting and others principles concerning a model are going to be tackled in the section 3 untitled :'Thinking in machine learning'.
  
\subsection{Summary concerning the tools and frameworks}
The completion of this project necessitated the utilization of various dependencies, tools, frameworks. These resources were instrumental in realizing the project's objectives. Notably, they were categorized into two main areas: those integral to the core functionality and others relevant to the user side, distinguishing the back-end from the front-end. Moreover, the project involved tools for data analysis and predictive modeling. The structure of these tools is presented in the  table \ref{tableStructuring}  for more clarity.
  \\
\begin{table}
\begin{tabular}{|p{4.1cm}|p{4cm}|p{6cm}|}
	\cline{2-3} 
    \multicolumn{1}{c|}{} &  \textbf{Tools}   &  \textbf{Roles}  \\
	\hline
\multirow{11}{4cm}{\textbf{Data analysis and Machine Learning libraries}}	& \textit{Numpy} & Scientific computing library   \\ 
	\cline{2-3} 
	& \textit{Panda}  & Data manipulation and
	analysis library  \\ 
	\cline{2-3} 
	& \textit{Matplotlib}  & Python library used for 2D and 3D data visualization   \\ 
	\cline{2-3}  
	& \textit{Seaborn} & Another Python library for statistical data visualization, built on \textit{matplotlib}.\\
	\cline{2-3} 
		& \textit{Scickit-learn}  & Machine learning machine learning library \\
	\hline
	\multirow{5}{4cm}{\textbf{Programming Languages (\textit{Front-end} and \textit{Back-end})}}& \textit{Django} Python & Python framework for developing web applications and \textit{APIs}\\ 
	 \cline{2-3}
	 & HTML & Maker of web pages by providing
	 its structure and content \\ 
	 \cline{2-3}
	 & CS & Styling the content\\ 
	 \cline{2-3}
	 & JS & Rendering web pages interactive and dynamic \\
	 \hline
	
\end{tabular}
\caption{Structuring the tools} 
\label{tableStructuring}
\end{table} 
\section{Description of the methodology and approach}
Choosing a right methodology including approaches and methods to use, is an important task. However, the nature of environment defers and leads to challenging evaluation before determining the fit one. Thus, since this project investigates in Machine Learning project, the appealing methodology chosen for its achievement is inspired from MLOps(ML Operating systems).

\subsection{\large  ML operating systems} 
ML endeavors aim to deal with their projects from development till the production step. However, a large number of projects based on ML don't reach the final step according to their expectations. Actually, the paradigm of MLOps came to face this issue \cite{kreuzberger2023machine}. For that, it is a set of principles, best practices and concepts, and development culture that provide an end-to-end machine learning development process to design, build and manage reproducible, testable and evolvable ML-powered software. \\

Furthermore, MLops involves the union of many disciples, including Machine learning, software engineering (concerning DevOps too), data engineering as clarified in figure \ref{fig:mlopsdisciplines}.
\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{../../../Books/Report/MLopsDisciplines}
	\caption{Intersection of disciplines in MLops}
	\label{fig:mlopsdisciplines}
\end{figure} 

\textbf The {Machine learning part} is dealt by \textbf{data scientists} and have the principle role of building the models that address the business question or needs. While \textbf{Data Engineering} is conducted by data engineers making sure that data pipelines which are the core of the ML model life cycle, are in turn and clean. Next, the \textbf{Software engineering}, encompasses \textbf{software engineers} not highly concerned by the machine learning model building since they bring ML problem into a well-engineered product (into applications). 
In the other hand, \textbf{DevOps} is directed by DevOps engineers who bridges the gap between development and operations, granting that updates are continuously integrated and the development is still pursued (referring to \textit{CI/CS}, Continuous Integration et Development ). \\

All these disciplines work together to follow the ML life cycle, demonstrating that ML endeavors are on ongoing process within the same project for ensuring that it is more impacting in terms of results. As shown in the figure \ref{fig:mllifecyle} the ML life cycle follows several steps resumed in the following points :
\begin{itemize}  
	\item Definition of the object and specification 
	\item Data Collection 
	\item Preparation and exploratory Data Analysis
	\item Model training and selection 
	\item Model evaluation
	\item Model Deployment 
	\item Model Monitoring 
\end{itemize} 
\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{ml-engineeringInspiration}
	\caption{Machine Learning Workflow}
	\label{fig:mllifecyle}
\end{figure}
\subsection{Definition of the object and specification}
The definition of the object and specification is start stage where details about the problem is given in the clear way for facilitating the ML development. This task is associated to Business stakeholders who define the goal pursued. 

\subsection{Gathering data}
At this step, raw data are collected from various data pipeline(sources) depending on interests of project and environment. Thus, data's source can be :  

(a) Databases, including data structured in relational databases like SQL and NoSQL databases. Actually, databases is mostly preferable source since they give structured data well-suited for analysis;

(b)  Files (CSV, Text Files). This mean is highly used. It requires important analysis of the content for being aware of how and what part to extract (as spreadsheets for Excel), separated values (as for CSV files).

(c) APis. They are also the common use for structured data( in JSON, XML format) , leveraging web services for providing data to third parties or internal parties of the system \cite{dataScienceMLeric}.

(d) Web Scrapping. This mean refers to techniques used for collecting data from web sites (sometimes illegally) and structure it into spreadsheets, CSV or others simple means \cite{sirisuriya2015comparative}. It's more appreciated when data are not available from the APIs. That being, some applications performs like 

(e) Images and videos : Images, videos are generated from cameras, satellites and other imaging devices. Thus, they are useful in specific tasks of ML like classification of taxonomy videos \cite{reviewVideo2020}. 
Many others means exist like surveys and questionnaires, audio data, however the guidance is problem to address. 
\\
Once the data are collected, it stored in \textbf{dataset} where they are subjected to manipulation and analysis.
\subsection{Data preparation and exploratory}
The preparation refers to two main tasks : The data pre-processing and data exploration . The data pre-prepocessing involves the learning of nature of the data and its characteristics, types format and quality. It can result to the cleaning of data if it noticed that they are unnecessary or tidy for the analysis. In all cases based on preparing data, can be called \textbf{data wrangling} \cite{furche2016data} process.

In the other hand,\textbf{ the exploratory data analysis} process aims to receive collected and cleaned data and facilitates the visualization that can be helpful in detection of outliers, identification of clusters and trends \cite{unwin2020data}. 
\subsection{Model training and selection}
When this stage is reached, it's obviously understood that the previous, ie. gathering, preparation, exploratory steps are already fulfilled. However, it is possible to go back there once again according to the analysis requirements.
What's happen during the training and testing stages ?
Since all clean data are found in a dataset containing features in columns and data values in rows, the class based on choices made for the comfortable algorithm can be subsequently used to generate a model. This model is able to learn relationships and patterns within the data, is what we call \textbf{'training'}. 
Thus, the selection of suit ML algorithm is more competitive involving studies, testing. However it is more lead by the kind of problem we wish to solve, the number of features and its types, the kind of model that would suit the data more the best \cite{wang2016machine}. 

So, according to those principles, here are the types of ML algorithms used:


\subsubsection{(a) Supervised models}  
The Supervise ML algorithms are one of algorithms often  used in intelligent systems. Their manner of functioning is this: They get as inputs the data related to the features, then they map them  with desired outputs (the output is input's entry). \\

Thus, before moving on the training step for finally getting the model, all the data are placed in a dataset where each entry represents \textbf{the feature vector} while the output defines \textbf{the target} or the answer gotten from the prediction. In addition, all the dataset entry values are considered as \textbf{feature matrix} and the columns are normally the \textbf{features}. \\

Therefore, for reaching a high score of precision, the prepared dataset can be split in tree parts : A part for training, used for the instructing the model, allowing it to learn patterns and relationships within the data; Second part for testing, used for testing if the model learns suitably the data provided to it, however data used for this stage are the than contained in a dataset; then for making sure that the model is able to learn from unseen data, another part is generated from the same dataset by strictly considering the data as unknown. \\

A part from spiting the dataset, several tasks are used by the supervised ML models for solving problems, as follows:

\textbf{- Classification tasks}: These tasks involve categorizing input data into predefined classes or labels \cite{osisanwo2017supervised}. The model learns to assign the correct category to new data based on patterns learned during training. Thus, they can are used for: Spam detection for classifying whether a message is a spam or not; Sentiment analysis for determining the social media posts text which can be positive, negative or neutral; image classification for identifying objects or categories in images.

\textbf{- Regression tasks}  : Regression tasks focus on prediction continuous numeric values based on input features. Besides, they are used to establish relationship among those features \cite{maulud2020review}. The algorithms based on these tasks are able to predict : The house prices, stock prices even the temperature forecasts.

\textbf{- Object Detection and Recognition} :  The algorithms related to these tasks are used to develop which are able to locate and identify objects within images and videos. They are often used in autonomous vehicles and surveillance systems and crops detection diseases \cite{bai2020object} .

\textbf{- Natural Language Processing (NLP)}  : This task involves the algorithms generating models which are wonderful in process of translating text form one language to another and identifying entities (e.g, names, locations) in text \cite{razno2019machine} . 

\textbf{- Time series Forecasting} : The model developed here, predict future values based on historical time-order data, with applications in in finance, weather forecasting (eg. prediction of wind speed \cite{langkvist2014review}), and demand prediction.

\textbf{- Speech Recognition} : Here, the concerned models able to convert spoken language into text, for enabling voice assistants and transcription services \cite{jiao2021spoken}.

\textbf{- Deep learning} : The uniqueness of models designed for this task lies in their capacity to delve deeply into extensive and intricate data, including unstructured forms such as images, text, and audio. These models possess the remarkable ability to autonomously learn complex hierarchies, enabling them to perform a wide range of tasks \cite{janiesch2021machine}.
\\

To gain deeper insight into the structure and impact of algorithms based on their tasks, the table \ref{whichTaskWhichModel} provides a more meaningful overview of some of it. 
\renewcommand{\arraystretch}{1.2}
\begin{table}[h]
	\begin{tabular}{p{3cm}p{5.6cm}p{6cm}} 
		
		\toprule 
		Tasks & Algorithms & Genre and problem solving examples    \\
		\toprule
		Classification & Naive Bayes, Logistic Regression, Support Vector Machine (SM), Random Forest, Decision Trees, etc. & Categorize input data into predefined classes or labels. 
		
		E.g: Sentiment Analysis. \\
		
		Regression & Linear Regression, Polynomial Regression, Lasso Regression, Ridge Regression, Suport Vector Regression (SVR), etc. & Predict continuous numerical output value.
		
		E.g: House prices forecasting.
		\\
		
		Object Detection & Convolutional Neural Networks (CNNs), etc. & YOLO (You Only Look Once). 
		
		E.g: Self-driving cars\\
		Natural Language Processing (NLP) & RNNS(Reccurrent Neural Networks), LSTMs, etc. & Undestand human language.
		
		E.g:  Language translation.\\
		Time series Analysis & ARIMA(Autoregressive Integral Moving average), Exponential Smoothing methods, Seasonal Decomposition of Time Series (STL), etc. & Predict future values in a time series.
		
		E.g: Weather prediction.
		\\
		Deep Learning & RNNs, NLP, GANs(Generative Adversarial Networks), etc. & Learn hierarchical representations from data, for deep prediction 
		
		E.g: Image recognition.
		\\
		\bottomrule
	\end{tabular} 
\caption{Which task for which Supervised Machine learning algorithm} 
\label{whichTaskWhichModel} 
\end{table} 

\subsubsection{\textbf{(b) Unsupervised Models}} 
Unlike supervised models which learns from the labels data, the unsupervised models are trained on unlabeled data. Their particularity lies in their ability to learn from complex and large amount of data. Their best goal is to find hidden patterns, structures or relationships within the data even though they are not proportional. Therefore, they are categorized as non-linear models  \cite{khanum2015survey}.\\

However, for professionally dealing with complex dataset containing linear or non linear, supervised and unsupervised models can be combined. In this case, the unsupervised can be mostly used to reduce the dimensionality of data before applying and the supervised model for linear regression prediction for example. This approach let profit from all both method's advantages \cite{liu2000interactive}.\\ 

The common tasks concerned by this ML type include : clustering, dimensionality reduction, data compression, topic modeling and many others. \\

\textbf{Clustering task}, is the fundamental task of this type. It is helps to group data into sensible grouping objects according to similarities and  characteristics. It obvious that all any class if pre-selected, or fore-grouped. Thus, the problem like : Grouping customers into clusters based on their purchasing behavior for market segmentation.\\

Subsequently, \textbf{the dimensionality reduction tasks} involves  model's techniques simplifying  the dataset to train while preserving its essential characteristics, that can be helpful to improve the performance of the model. The challenge of this method when projecting data \cite{sorzano2014survey}, is to choose the right techniques, we mean parameters, features,...  The related solutions include : Data processing for machine learning tasks, visualization of high-dimensional data in fields like biology and natural language processing. 

In addition, \textbf{the data compression} task, is inspired from the recent advancements in the field of information technology resulted to the generation of huge amount of data at each and every second \cite{jayasankar2021survey} . Therefore, the need of having a method able to eliminate data redundancy and irrelevancy is observed. This need becomes fulfilled by the unsupervised models allowing further analysis and pre-processing.\\

Furthermore, \textbf{the topic modeling}, is due to the growth of texts in new environment like internet, where informations including news headlines, web pages, questions/answers are published \cite{qiang2020short}. Thus, analysis of text becomes essential. It is performed by unsupervised models focusing on language, frequent words, and many others parameters.\\

The table provides a comprehensive overview of these tasks and the associated algorithms employed to achieve them.

\renewcommand{\arraystretch}{1.2}
\begin{table}[h]
	\begin{tabular}{p{3cm}p{5.6cm}p{6cm}} 
		
		\toprule 
		Tasks & Algorithms & Genre and problem solving examples    \\
		\toprule
		Clustering & K-Means Clustering, Hierarchical Clustering, DBSCAN, etc. & Group data points into clusters based on similarity, without prior knowledge of group labels
		
		E.g: Customer Segmentation . \\
		
		Dimension Reduction & PCA(Principal Component Analysis), t-Distributed Stochastic Neighbor Embedding, etc. & Reduce the number of features(dimensions) in a dataset and retain essential information.
		
		E.g: Image compression .\\
		Topic Modeling & LDA(Latent Dirichlet Allocation), Non-Negative Matric Factorization(NMF), etc. & Discovering latent topics within a collection of documents
		
		E.g: Content Recommendation \\
		
		Data compression & PCA & Preprocess daat to make data more manageable.
		
		E.g : Data storage.\\
						
		\bottomrule
	\end{tabular} 
	\caption{Which task for which Unsupervised Machine learning algorithm} 
	\label{whichTaskWhichModelUnsupervised} 
\end{table}  


\subsection{Model Evaluation}
The evaluation of machine learning models is a critical aspect of the model development process. Its helps to assess the model's performance, its ability to make accurate predictions, and its generalizations to unseen data \cite{raschka2018model}. \\

Therefore, various techniques are implemented to assess that the model is worthy to be used. However, it defers from problem or models types performing a certain tasks. Thus, two main techniques are used : \textbf{cross-validation and the measure of metrics}.\\

The cross-validation is used to measure the performance of the model. For performing this, the methods such as: \textbf{\textit{K-Fold}} Cross-validation are used. This one; divides the dataset into \textbf{'k' subsets }of approximately equal size, then the model is trained and tested \textbf{'k' times} using each of the k subsets as the test set exactly once while the remaining \textbf{subsets are used for training}. The others cross-validation are : Stratified cross-validation and Leave-One-Out Cross-validation (LOOCV). \\

A part from cross-validation, we have the measure of metrics as one of highly techniques used to estimate the model accuracy. However, the metrics depends on tasks performed by the model: \\

For classification tasks, it used what we call the confusion matrices to evaluate errors in classification problems.
As shown in table \ref{tab:confusionMatrixTable}, \cite{raschka2018model} , this method focuses on evaluating the ability of model to classify instances into different categories. Thus, there are \textbf{True Positives} (TP) which are instances that were correctly predicted as belonging to the positive class; \textbf{True Negatives} (TN), which are instances that were correctly predicted as belonging to the negative class. In the same way, \textbf{False Positives} (FP) are instances that were incorrectly predicted as belonging to the positive class. While the \textbf{False Negatives} (FN) are instances that were incorrectly predicted as belonging to the negative class \cite{maria2016performance}. 
\begin{table}[h]
	\centering
	\begin{tabular}{|p{3cm}|p{3.5cm}|p{3cm}|p{5cm}|} 
		\cline{2-4}
		\multicolumn{1}{c|}{}& \multicolumn{3}{c|}{\textbf{Observations}}   \\
		\cline{1-4} 
		\textbf{Predictions}	& \textbf{Actual Negative} & \textbf{Actual Positive} & \textbf{Total}\\
		\hline
		Predicted Negative & True Negatives (TN) & False Negatives (FN) & Total of Negatives predicted  (FN+TN)\\
		\hline
		Predicted Positive & False Positives (FP) & True Positives (TP) & Total of Positives predicted  (FP+TP)\\
		\hline
		Total & Total of Negatives observed & Total of Positives Observed & Size of all total data\\
		\hline
	\end{tabular} 
	\caption{Confusion Matrix}
	\label{tab:confusionMatrixTable}
\end{table}

Therefore, those parameters give sense to several metrics as following : 

- \textbf{Accuracy}, which is the proportion of correct predictions over the total number of predictions (Eq.\ref{eq:accuracy}) : 
¨
\begin{equation} 
\label{eq:accuracy}
	ACC = \frac{TP+TN}{TP+TN+FP+FN} 
¨\end{equation} 
- \textbf{The Precision}, is the fraction of true positive predictions(TP) out of all positive predictions (Eq.\ref{eq:precision}). 
\begin{equation}
	\label{eq:precision} 
	Precision = \frac{TP}{TP+FP}
\end{equation}  
- There's the \textbf{Recall} also called Sensitivity, True Positive Rate or TPR; measuring the fraction of true positive predictions out of all actual positive instances (Eq.\ref{eq:recall}). 
\begin{equation}
	\label{eq:recall}  
	Recall = \frac{TP}{TP+FN}
\end{equation} 
- There's too, the \textbf{F1-Score}, which is the harmonic mean of precision and recall since it balances precision and recall (Eq. \ref{eq:fscore}). 
\begin{equation}
\label{eq:fscore}
F1Score = \frac{2 \cdot (Precision \cdot Recall)}{Precision + Recall} = \frac{2VP}{2VP + FP + FN}
\end{equation}

For sure, all the metrics are convenient for a given situation or problem to solve and have to be high. Most of the time the percent of 80 \% is high, while in others 90\% with the possibility of taking care of false positives.  \\

Besides, the \textbf{Regression Tasks} present too the a lot metrics among them there are \cite{coursAnalyseDonne3} : 

- \textbf{Mean Absolute Error(MAE)}, it measures the average absolute difference between the predicted values and the actual values. 

\begin{equation}
	\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\end{equation}

- \textbf{Mean Squared Error (MSE)}, it measures the average of the squared differences between predicted and actual values.
 Every time a metric's value is lower, it indicates that the model's predictions are closer to the actual values. 

\begin{equation}
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\end{equation}
- \textbf{R-squared (R²) or Coefficient of Determination}, it measures the proportion of the variance in the target variable that is explained by the model. 
\begin{equation}
	R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
\end{equation}

In terms of interpretation in a programming language , case of Python, these metrics are found in the packages : \textit{sklearn.metrics}.\\

\subsection{Model Deployment and monitoring}
Machine Learning Model Deployment refers to the process of taking a trained machine learning model and making it available for use in real-world applications. Therefore, several means are used: Primarily, we have to choose the environment. Is it going to be integrated in a cloud platforms, or applications, edge devices ? Actually, it depends on interests. Some developers can decide to create an \textit{API} that allows software components to interact with clean model built as Web Service \cite{singh2021deploy}. For instance in python the \textit{frameworks} like \textit{Flask, Django} since they offer the \textit{RestApi} services. \\

However, the model required to be integrated in a file for insuring the integration whether in the cloud or others services as emphasized above. Thus, the packages like : \textit{joblib} \footnote{joblib: \url{https://joblib.readthedocs.io/en/latest/index.html}} . Actually, \textit{Joblib} is used for serialization(saving and loading) Python objects. It's convenient for task like \textbf{parallel computing and disk caching of functions}  \cite{faouzi2020pyts}.\\

In part from, the model should be integrated in a Continuous integration and Continuous Deployment (CI/CD) (task of DevOps developer) to facilitate \textbf{monitoring} and updating, even the easy deployment. \\

Furthermore, the security of a model is so important too. Hence, the model's container should be protected in a safe area. 

\section{ML Algorithms and Scikit-learn}  
ML Algorithms are numerous, however some of them are known to be utilized in several tasks as shown above, both unsupervised and supervised algorithms are supported in Python Scikit-learn package, whereas the logic behind its implementation is more important since it's allows the understanding and optimization of solutions. Thus, the following lines summarize the common algorithms and how they can be implemented in Scikit-learn.\\

Since two main types of category are notable in analysis which are : categorical(qualitative) and quantitative, the presented algorithms are set in the same approach for more clarification.

\subsection{Predict Qualitative feature} 
The prediction's result for this kind of data, involves an output which is commonly from binary, text, orders classes. Thus, we have : 
\subsubsection{Naive Bayes} 
The Naive Bayes is a classification algorithm based on Baye's theorem, which is a fundamental concept in probability theory \cite{maria2016performance}. 
Actually, the Naive Bayes formula is given like this (Eq.\ref{naiveProbaEquation}) : 
\begin{equation}
	\label{naiveProbaEquation}
	P(A|B) = \frac{P(B|A)  \cdot  P(A)}{P(B)}
\end{equation}
In this equation \ref{naiveProbaEquation}, the event A represents the existence of \textbf{class} A; while the event B intervenes when the \textbf{input} B is given. Thus, we read the probability of having the class A when the input B is given. 

\noindent By assuming that we have more than one input, the formula will look like this : 
\begin{equation} 
	\label{probabilitieEqualNaive}
	P(A | B_1, B_2, \ldots, B_n) = \frac{\textbf{P(A)} \cdot P(B_1 | A) \cdot P(B_2 | A) \cdot \ldots \cdot P(B_n | A)}{P(B_1, B_2, \ldots, B_n)}
\end{equation}  
\begin{equation}
	P(A | B_1, B_2, \ldots, B_n) \propto P(A) \cdot P(B_1 | A) \cdot P(B_2 | A) \cdot \ldots \cdot P(B_n | A)
\end{equation}
Since in equation \ref{probabilitieEqualNaive}, the denominator is in the second member is independent from the class (A), we just substitute the comparison sign by : \textbf{proportional to} sign. Hence, the final formula used in the implementation is like this :
\begin{equation}
P(A | B_1, B_2, \ldots, B_n) \propto P(A) \prod_{i=1}^{n} P(B_i | A)
\end{equation}

Great,the \textit{Naive Baye} in sckit-learn will consequently look like this : \newline
\textbf{\textit{sklearn.naive\_bayes.GaussianNB()}}. As it it is \textbf{\textit{GaussianNB}} type, it means that the features's values affected to the model are continuous instead of being discrete, normally distributed. The model can be \textbf{\textit{Multinomial}} too, when it is interested in discrete's values, it means the frequency of words.
\subsubsection{Logistic Regression} 
Logistic Regression is a binary and multi-class classification algorithm that models the relationship between inputs features and the probability of a particular event occurring \cite{rymarczyk2019logistic}. For transforming a linear combination of features into probabilities between 0 and 1, it uses the \textit{sigmoid} function as follows :
\begin{equation} 
\label{sigmoidFunction}
\sigma(z) = \frac{1}{1 + e^{-z}}
\end{equation} 
The \textbf{z} variable is the linear combination of input features and coefficients. 

A part from that, \textit{LR} model uses techniques like regularization for preventing over-fitting, solver for optimizing problem, etc.
To get started with it, with scickit-learn, we just use the class :
\textbf{\textit{sklearn.linear\_model.LogisticRegression()}} 

\subsubsection{Decision Trees} 
As all supervised learning algorithms presented above, Decision Trees(DTs) are a non-parametric method used for classification and regression. Its prediction's ability is based on the if-then-else learn's possibilities \footnote{sckit-learn: \url{https://scikit-learn.org/stable/modules/tree.html} }. Actually, the tree is composed by the nodes and branches. Each node represents features in a category to be classified and each subset defines a value that can be taken by the node \cite{charbuty2021classification}.  
\subsubsection{Support vector machine (SVM)} 
SVMs are a class of powerful machine learning algorithms used for classification and regression. They aim to find the optimal hyperplane that maximally separates data points of different classes while maximizing a margin. It's a favorite solution for high-dimensional data\cite{memoireUniversitySpam}. Its class look like this : \textbf{\textit{sklearn.svm.SVC()}}

\subsubsection{\textit{KNeighborsClassifier}}
The \textbf{\textit{KNeighborsClassifier}} is ML algorithm based on \textit{k-Nearest Neighbors} (K-NN) principles. It classifies data points by finding the \textbf{\textit{k}} nearest from the training dataset and assigning a class label based on majority voting. For measuring similarity between data points, it utilizes the distance metric \cite{nayak2022study}. 
To get started with it, we need to use the class\textbf{ \textit{sklearn.neighbors.KNeighborsClassifier()}}. 

\subsubsection{Random Forest}
Random Forest is an updated algorithm version of  decision trees. It combines trees to enhance predictive accuracy in classification and regression tasks. It employs \textit{bagging} \footnote{Bagging : Boostrap aggregating, improves the accuracy and robustness of models}  to create diverse subsets of the training data and random feature selection for each tree\cite{lin2017ensemble}. Consequently, the overfitting is reduced and generalization is enhanced. \\

A part from optimizing prediction's result, it is reputed for being scalable and applicable to both small and large datasets. 

To get started with it in Python, scikit-learn library we just the following class : \newline \textbf{\textit{klearn.ensemble.RandomForestClassifier()}}. 
\subsubsection{Neural Networks (ANN)}
The Artificial Neural Networks (ANNs) are ML models inspired by the human brain. They consist of interconnected nodes(neurons) \cite{boateng2020basic} organized into layers: input, hidden, and output. They excel in learning complex patterns from data through a process called \textbf{\textit{backpropagation} \cite{lillicrap2020backpropagation}}. 

To get started with it, we can just use the simple variant's class : \newline \textit{\textbf{sklearn.neural\_network.MLPClassifier()}}.

\subsection{Predict quantitative feature} 
This section includes algorithms with the objective of prediction numerous values, which can be either discrete or continuous. Therefore, we have :
\subsubsection{\textit{ElasticNet}}
ElasticNet is a crucial tool in building of robust and generalized models. It uses regularization techniques in the linear regression, allowing to address the limitations of L1(Lasso) and L2(Ridge) regularization methods \cite{johnsen2020elastic}. 
To get started with it, we just implement the class : \textit{\textbf{sklearn.linear\_model.ElasticNet()}}

\subsubsection{\textit{Gradient Boosting Regressor (GBR)}}
\textit{GBR} is an improved version of \textit{gradient descent}, making more accurate and good at minimizing a loss function, since it is an ensemble ML algorithms used for regression tasks. 
The other particularities of this model is the configuration's parameters supporting in \textbf{\textit{boosting settings, model tuning}}. 
To get started we wih \textit{GBR}, we just use the following class : \textit{\textbf{sklearn.ensemble.GradientBoostingRegressor()}}

\subsubsection{\textit{Regression Lasso/Ridge}}
 All both (\textit{Ridge/Lasso}) are updated version of \textit{Linear Regression} algorithm, however they are used for avoiding overfitting (regularization) of a model. Therefore, \textit{Lasso} uses \textit{L1} regularization penalty added to the \textbf{loss function}. 
 In the other hand, the Ridge type uses \textit{L2} regularization integrating the loss function based on the square of the model's coefficients \cite{hazan2011optimal}. 
 Actually the loss function is used during the model training stage for quantifying and minimizing the error between the model's predictions and the actual target values for a given data \cite{wang2020comprehensive}.  
 
 \subsubsection{\textit{KNeigboursRegressor}}
 The \textit{K-Nearest Neighbors Regressor} often abbreviated as \textit{KNeighborsRegressor} is a member of the \textit{K-Nearest Neighbors(KNN)} family, including classification and regression variants. KNeighborsRegressor is particularly well-suited for solving problems where the target variable is continuous and requires predicting a numeric value, for instance the prediction of crime using KNN Regressor \cite{alsayadi2022improving}.
 
 In \textit{scikitlearn}, in order to deal with this model, it has to be instantiated from the following class : \newline \textit{\textbf{sklearn.neighbors.KNeighborsRegressor()}}. 
 
\section{Techniques used for optimizing the model} 
Several techniques are employed to ensure that the implemented model achieves a high level of accuracy. While we've touched upon these concepts indirectly in preceding discussions, it's worth highlighting them for more clarity. Thus, we have:
\subsection*{Regularization} 
The regularization is technique used upon a model to enhance the generalization ability of the models. The methods like Regression \textit{Ridge}, \textit{Lasso},  \textit{Elastic} can be used for that concern \cite{mlwithpython}.  For instance : \textit{L1} regularization called \textit{Lasso} can be used to drive all  owner's name values feature to exactly zero, since it's \textbf{might have any impact on price}, concerning the house price prediction project.
\subsection*{Ensemble models}
The ensemble methods combine the predictions of multiple individual models(base models) to produce a more accurate and robust prediction \cite{kabari2019comparison}. Therefore, it raises new methods such as :
\subsubsection{(a) \textit{Bagging} (Boostrap Aggregating)} 
Actually, Bagging involves the process of training multiple instances of the same base model on different subsets of the training data and finally takes the mean of all the predictions. The case explaining this is how Random Forest works as ensemble method and the Decision Trees as its base models. The examples of the models's genre are : \textbf{\textit{AdaBoost}} and \textbf{\textit{Gradient Boosting}}.
\subsubsection{(b) \textit{Boosting}} 
The boosting process focuses on training multiple base models sequentially. Each model in the ensemble corrects the errors of the previous one, with more emphasis on misclassified data points. Actually, the correction's goal is the achievement of accurate prediction rule \cite{praveena2017literature}.

\subsubsection{(c) \textit{Voting}} 
Voting ensembles is more special. It combines the predictions of multiple \textbf{base models by taking a majority vote}(for classification) or averaging(for regression) to make the final prediction \cite{kabari2019comparison}. However, if the model is not optimized, it is might be biased. The model like \textit{VotingClassifier, VotingRegressor} are commonly used. 
\subsection*{\textit{Dimensionality Reduction}}
Dimension reduction techniques focus on reducing the number of features (dimensions) in a dataset while preserving or maximizing the relevant information. They are also used to address issues associated with high-dimensional data, such as the curse of dimensionality, overfitting, and computational complexity. Thus, we discuss \textbf{feature selection} when we aim to retain the relevant and informative features and \textbf{feature extraction} when data is projected into a lower-dimensional space \cite{cunningham2008dimension}. In practice, the original data is first transformed and then technically selected to create a new representative dataset. 

The common models used for dimensionality reduction include techniques such as Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA).
\subsection*{\textit{Hyperparameter Tuning}} 
Hyperparamater tuning, often referred to as hyperparameter optimization, is the process of systematically searching for the best combination of hyperparameters for a machine learning model  \cite{chauhan2020detection}.\\

Actually, they are settings that are not learned from the data but are set prior to training and setting a model. Examples of hyperparameters include the learning rate in neural networks, the depth of a decision tree, the number of neighbors in k-nearest neighbors, or the regularization strength in linear models. In scikit learn model called \textit{\textbf{GridSearchCv}} can be used to test the convenient parameters. 

\subsection*{\textit{Normalization and standardization}} 
Both normalization and standardization are preprocessing techniques commonly used in machine learning and statistics to transform data before feeding it into a model. They serve different purposes but aim to make data more suitable for modeling and analysis. \\

The normalization is the process of rescaling data to a common scale, typically between 0 and 1. It's useful when features we have different ranges, and we want to bring them to a uniform scale \cite{ali2014data}, \cite{elen2021standardized}. Common methods for normalization include Min-Max scaling. 

The formula used for Min-Max scaling is : 
\begin{equation}
	X\_normalized = (X-X\_min)/(X\_max - X\_min)
\end{equation} 
Where \textit{X} is the original value, \textit{X\_min} is the minimum value in the dataset, and \textit{X\_max} is the maximum value.

The standardization, on the other hand, transforms data into a standard normal distribution with a mean of 0 and a standard deviation of 1 \cite{ali2014data}. It's particularly useful when the features follow a normal or near-normal distribution, and the absolute values and relationships between them are important. Practically, it involves subtracting the mean from each value and dividing by the standard deviation. 
\begin{equation}
	XStardardized = (X - mean) / standardDeviation
\end{equation}
In scikit learn the model like \textit{PCA} can be used to compute this formula on a dataset.
\section{Summary}
In this chapter, the primary focus is on the theorical aspects of the current work. It starts by reviewing related works to identify potential enhancements offered by this work. Subsequently, it explores the tools employed for realizing the proposed solution and delved into the methodological approaches adopted.\\

Regarding tools and techniques, they classified into two main categories. The first category comprises the tools utilized for constructiong machine learning models, which include algorithms implemented in Python using various libraries.
The second category involves application tools, such as web technologies like HTML, CSS, Javascript, and the Django Framework. These applications tools are instrumental in creating user interfaces for consuming a web service.\\

Furthermore, this chapter delves deeply into the \textit{MLops}(Machine Learning Operations) methodology employed to make a machine learning model consumable and adaptable to evolving needs. During this phase, it explores the various steps necessary to develop a robust model with high precision and accuracy.
\chapter{Application of the methodology and results with analysis}   
\section{Introduction} 
This chapter delves into the practical aspect of the present work. It demonstrates how the discussed methodology has been applied and how it culminates in proposing a solution for the identified problems. In addition, it addresses how individuals contribute to its completion, the constraints associated with its deployment and usage, as well as the encountered limitations. Furthermore, it explores potential perspectives for future research by other researchers. 
\section{Participants}
\subsection{Survey participants}
This work couldn't have any sense without the collaboration, assistance of the volunteers who are almost students and friends at camp, members of social groups. They just provided to us the unwillingly messages as much as they were able. 
\subsection{Team work structure} 
The present project witnessed a dynamic partnership between the authors and an enthusiastic academic team, consisting of both the supervisor and director. They played pivotal roles in guiding the project, offering valuable insights into data collection methodologies, and suggesting techniques to achieve our goals effectively. Beyond the academic community, we also benefited from expertise of online scientists who provided valuable advise and coaching to ensure our solutions were relevant and impactfull.

\section{Data collection strategy } 
Recognizing the pivotal role of data at the heart of our solution, an extensive investigation into data collection became imperative. Our fist approach to streamline this process was by harnessing the power of the online realm. Trough a dedicated website, we deployed a user-friendly survey form; which individuals could readily complete and submit. Visualize this in figure \ref{fig:spamform} in the annexes.
Therefore, all the data collected have been stored in a database. To really collect it for further investigations, it was just question of clicking on a download button in the website as shown in figure\ref{fig:collectdata} in the annexes.

Furthermore, in keeping with the common practice among data scientists, who often employ online datasets for their experiments and analysis, this work also delved into \textbf{a dataset readily available on} \textit{Kaggle}, which allows this work to tackle broad languages even though they are not highly spoken in case study area. \footnote{https://www.kaggle.com/ : Actually, \textit{Kaggle} is an advanced online community for networking works and storing data mostly used for analysis project.} 

\section{Application of the methodology} 
Pursuing the ML operating systems methodology: From the gathering step to model deployment, this section presents all the steps which lead to the model worthy to make predictions. 
\subsection{Gathering data} 
Collecting technically data required to know and its sources. Thus, pursuing that in \textit{jupyter} notebook, we firstly import the essential dependencies like this :
\begin{lstlisting}[style=stylejupyter]
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression 
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt 
import seaborn as sns
import time
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC 
from sklearn.linear_model import LogisticRegression 
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import VotingClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import roc_auc_score,accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
import numpy as np 
import pandas as pd 
import detectlanguage  
from sklearn.feature_extraction.text import TfidfVectorizer
\end{lstlisting}
Hence, the pandas alias created allows to gather data, both based on two languages category, one highly concerns strange languages (French, English), and the other local (Swahili). Therefore, we used the following code : \\

\begin{lstlisting}[style=stylejupyter]
# Multilinguages data
messageInMultiLang = pd.read_csv('data-en-hi-de-fr.csv') 

#local data
messageInSwahili = pd.read_csv('swahiliDataset.csv')
\end{lstlisting}

\subsection{Data preparation and exploratory} 
During this step, we dropped null values, duplicates, reformatted data types and finally we created plot for visualizing data following the present steps and techniques:\\

- Check the columns inside of the data:
\begin{lstlisting}[style=stylejupyter]
In [43]:# columns in multilingual dataset 
	print(messageInMultiLang.columns) 
	print(messageInSwahili.columns) 
	
	Index(['labels', 'text', 'text_hi', 'text_de', 'text_fr'], dtype='object')
	Index(['labels', 'message'], dtype='object')
\end{lstlisting}
- Extract the valuable features:
\begin{lstlisting}[style=stylejupyter]
	#extract english messages
	messageInEnglish = messageInMultiLang[['labels','text']] 
	
	#extract french messages
	messageInFrench = messageInMultiLang[['labels','text_fr']]
\end{lstlisting}

- Visualize the sample data :

(a) English language data:

\includegraphics[height=4cm,width=1\linewidth]{CollectImages/headViewsEnglish}\\

(b) French language data:

\includegraphics[height=4cm,width=1\linewidth]{CollectImages/headViewsFrench}

(c) Swahili Language data: 
 
\includegraphics[height=4cm,width=1\linewidth]{CollectImages/headViewsSwahili} 
\\

- Counting data:\\
(a) English Language data :

\begin{lstlisting}[style=stylejupyter]
pd.crosstab(index=messageInSwahili['labels'], columns='count') 
pd.crosstab(index=messageInFrench['labels'], columns='count')
pd.crosstab(index=messageInEnglish['labels'], columns='count')
\end{lstlisting}

The above code gives us results concerning raw data in our   all these pictures and using, all both datasets contained the data as follows in the table \ref{tab:rawdatacollected} : \\
\begin{table}[h]
	\centering
\begin{tabular}{|p{3cm}|p{3.5cm}|p{3cm}|p{4cm}|} 
	\hline
	Languages & Ham  & Spam & Total\\
	\hline
	English & 4825 & 747  & 5572  \\
	\hline
	French&  4825 & 747 & 5572  \\ 
	\hline
	Swahili & 117 & 15 & 132\\
	\hline 
\end{tabular}
\caption{Raw data counted before any wrangling} 
\label{tab:rawdatacollected}
\end{table}

- Checking null values :
\begin{lstlisting}[style=stylejupyter]
print(messageInEnglish.isna().sum()) 
print(messageInFrench.isna().sum()) 
print(messageInSwahili.isna().sum()) 
\end{lstlisting}  
This snippet code prints that any entry is empty.\\

- Dropping duplicated values :
\begin{lstlisting}[style=stylejupyter]
messageInSwahili=messageInSwahili.drop_duplicates() 
messageInFrench = messageInFrench.drop_duplicates() 
messageInEnglish=messageInEnglish.drop_duplicates()  
\end{lstlisting}
After dropping duplicated values, it is noticed that respectively 19, 438 and 415  Swahili, French and English entries were dropped. Thus, the illustration of clean data is in the table \ref{cleandatacollected}, also visualized in figure \ref{cleanEnglishEntrie},  \ref{cleanFrenchEntrie}, \ref{cleanSwahiliEntrie}.
\begin{table}[h!]
	\centering
	\begin{tabular}{|p{3cm}|p{3.5cm}|p{3cm}|p{4cm}|} 
		\hline
		Languages & Ham  & Spam & Total\\
		\hline
		English & 4516 & 641  & 5135  \\
		\hline
		French&  4494 & 640 & 5134\\ 
		\hline
		Swahili & 99 & 14 & 113 \\
		\hline 
	\end{tabular}
		\caption{Clean data counted after preprocessing} 
	\label{cleandatacollected}
\end{table}
 
\begin{lstlisting}[style=stylejupyter]
custom_colours = ['#ff7675', '#74b9ff']
labels = ['ham', 'spam']

def visualisationMsg(data, text):
sizes = data['labels'].value_counts().tolist()

plt.figure(figsize=(20, 8), dpi=227)

# Plot 1 - Pie Chart
plt.subplot(1, 2, 1)
plt.pie(sizes, labels=labels, textprops={'fontsize': 15}, startangle=140, 
autopct='%1.0f%%', colors=custom_colours, explode=[0, 0.05])
plt.title('Distribution of '+text)

# Plot 2 - Bar Plot
plt.subplot(1, 2, 2)
sns.barplot(x=data['labels'].unique(), y=data['labels'].value_counts(), palette='viridis')
plt.title('Bar Plot of Messages'+text)

plt.show() 
\end{lstlisting} 
\begin{lstlisting}[style=stylejupyter]
visualisationMsg(messageInSwahili, " Swahili messages") 
visualisationMsg(messageInFrench, ' French messages')  
visualisationMsg(messageInEnglish, ' English messages')
\end{lstlisting}
\begin{figure}[h]
	\centering
	\includegraphics[width=01\linewidth]{CollectImages/EnglishPlot} 
	\caption{Clean English entries }
	\label{cleanEnglishEntrie}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=01\linewidth]{CollectImages/frenchPlot} 
	\caption{Clean French entries }
	\label{cleanFrenchEntrie}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=01\linewidth]{CollectImages/frenchPlot} 
	\caption{Clean Swahili entries }
	\label{cleanSwahiliEntrie}
\end{figure}

- Feature engineering:

  During this process, we select the target feature, and transform it in the boolean format in which it can be more understood by the model. For that, we \textit{labelEncoder() model} as follows :
  \begin{lstlisting}[style=stylejupyter]
  	#Create the instance
  	modelLabelEncoder = LabelEncoder() 
  	
  	#Create a helpul function
  	def encondModel(dataMessage):
  	#encod the target feature
  	  return modelLabelEncoder.fit_transform(dataMessage['labels'])  
  	
  	#we create the y feature for each language
  	y_en = encondModel(messageInEnglish) 
  	y_fr = encondModel(messageInFrench) 
  	y_sw = encondModel(messageInSwahili)
  \end{lstlisting}

\subsection{Model selection and prediction}
The division of data in slices: Ones used for training and other for testing :
\begin{lstlisting}[style=stylejupyter]
#English dataset
x_train_en, x_test_en, y_train_en,y_test_en = train_test_split(messageInEnglish['text'],\
y_en, test_size=0.2, stratify=messageInEnglish["labels"])

#French dataset
x_train_fr, x_test_fr, y_train_fr,y_test_fr = train_test_split(messageInFrench['text_fr'],\
y_fr, test_size=0.2, stratify=messageInFrench["labels"])
#Swahili dataset
x_train_sw, x_test_sw, y_train_sw,y_test_sw = train_test_split(messageInSwahili['message'],\
y_sw, test_size=0.2, stratify=messageInSwahili["labels"])
\end{lstlisting}

The words : \textit{text, text\_ft, message} are names of messages values. Besides, \textit{stratify} parameter is optional, however used for ensuring that the distribution made remains consistent.
 
- Feature extraction:
 The current extraction involved a use of model which is able to convert text into numeric values understandable by the model. Thus, \textit{TfidfVectorizer} performed that tasks by even providing techniques that counts the frequency of word in a message. Hence, this model plays a pivotal role for each dataset:

\begin{lstlisting}[style=stylejupyter]
#function to automate the function Tfidf 
def featureExtractionModel(language):
return TfidfVectorizer(min_df=1, stop_words=language, lowercase =True)   
 	
#Convertion for englishs part
featureExtractionEn = featureExtractionModel('english')
 	
#Prevent stops and configure it for french
from spacy.lang.fr.stop_words import STOP_WORDS
featureExtractionFr = featureExtractionModel(list(STOP_WORDS))  
 	
#For swahili
featureExtractionSw = featureExtractionModel(None) 
 \end{lstlisting}
 After created the features extractions model for each language, now the transformation of test values were worthy to be done, as follows :
 \begin{lstlisting}[style=stylejupyter]
x_test_features_en = featureExtractionEn.transform(x_test_en)
x_test_features_fr = featureExtractionFr.transform(x_test_fr)
x_test_features_sw = featureExtractionSw.transform(x_test_sw)
 \end{lstlisting}  
 
 - Initialize models: These are certain models used classify:
 \begin{lstlisting}[style=stylejupyter]
 #Naive Bayes models
 BayesModelEn=MultinomialNB()  
 BayesModelFr=MultinomialNB() 
 BayesModelSw=MultinomialNB() 
 
 #SVM models
 SvmModelEn = SVC(probability=True)
 SvmModelFr = SVC(probability=True)
 SvmModelSw = SVC(probability=True)
 
 #Logistic regression models
 LogRegModelEn= LogisticRegression() 
 LogRegModelFr= LogisticRegression() 
 LogRegModelSw= LogisticRegression() 
 
 #DecisionTree models
 DecisionTreeEn = DecisionTreeClassifier()
 DecisionTreeFr = DecisionTreeClassifier()
 DecisionTreeSw = DecisionTreeClassifier()
 \end{lstlisting}

- Predictions based on each language model: 
\begin{lstlisting}[style=stylejupyter]
#Function to fit all the models
def modelReceiveFitting(model,x_train_features, y_train):
 fitmodel = model.fit(x_train_features, y_train)
 return fitmodel
\end{lstlisting}
Fitting each models as follows :
 
\begin{lstlisting}[style=stylejupyter]
#For english languages
BayesModelEn=modelReceiveFitting(BayesModelEn,x_train_features_en,y_train_en) 
SvmModelEn=modelReceiveFitting(SvmModelEn,x_train_features_en,y_train_en) 
LogRegModelEn=modelReceiveFitting(LogRegModelEn,x_train_features_en,y_train_en) 
DecisionTreeEn=modelReceiveFitting(DecisionTreeEn,x_train_features_en,y_train_en) 

#For french languages
BayesModelFr=modelReceiveFitting(BayesModelFr,x_train_features_fr,y_train_fr)
SvmModelFr=modelReceiveFitting(SvmModelFr,x_train_features_fr,y_train_fr) 
LogRegModelFr=modelReceiveFitting(LogRegModelFr,x_train_features_fr,y_train_fr) 
DecisionTreeFr=modelReceiveFitting(DecisionTreeFr,x_train_features_fr,y_train_fr) 

#For swhahili languages
BayesModelSw=modelReceiveFitting(BayesModelSw,x_train_features_sw,y_train_sw)
SvmModelSw=modelReceiveFitting(SvmModelSw,x_train_features_sw,y_train_sw) 
LogRegModelSw=modelReceiveFitting(LogRegModelSw,x_train_features_sw,y_train_sw) 
DecisionTreSw=modelReceiveFitting(DecisionTreeSw,x_train_features_sw,y_train_sw)
\end{lstlisting}
\subsection{Model Evaluation}

Once the model was fitted, it was time for evaluating the precision those models by two indicators which are : accuracy and ROC metrics. Therefore, this function performs that task for every model leveraged, and it was technically made as follows: 
\begin{lstlisting}[style=stylejupyter]
def makeRocVisualize(bayes, svm, lg,decision,x_feature, y_test,language):
	models = [
	(bayes, "Bayes Model"),
	(svm, "SVM Model"),
	(lg, "Logistic Regression Model"),
	(decision, "Decision Tree Model"),
	]
	
	plt.figure(figsize=(8, 6))
	
	for model, model_name in models:
	y_scores = model.predict_proba(x_feature)[:, 1]  
	# Probability estimates for the positive class
	fpr, tpr, _ = roc_curve(y_test, y_scores)
	auc = roc_auc_score(y_test, y_scores)
	
	print(model_name + " Accuracy score is :", accuracy_score(y_test, model.predict(x_feature)))
    print(model_name + " Area under the curve for ROC is :", auc) 

	
	plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})')
	
	plt.plot([0, 1], [0, 1], 'k--', linewidth=2)  # Random classifier line
	plt.xlim([0.0, 1.0])
	plt.ylim([0.0, 1.05])
	plt.xlabel('False Positive Rate')
	plt.ylabel('True Positive Rate')
	plt.title('ROC Metrics'+language)
	plt.legend(loc='lower right')
	plt.show()
	}
\end{lstlisting}
Using the function for each dataset chosen we got the following result :

(a) English model's score: 
\begin{lstlisting}[style=stylejupyter]
In [343]: makeRocVisualize(BayesModelEn,SvmModelEn,LogRegModelEn,DecisionTreeEn,x_test_features_en,y_test_en, "(English Data)")

#results 
Bayes Model Accuracy score is : 0.9641472868217055
Bayes Model Area under the curve for ROC is : 0.9893010232300885
SVM Model Accuracy score is : 0.9728682170542635
SVM Model Area under the curve for ROC is : 0.9972863661504424
Logistic Regression Model Accuracy score is : 0.9573643410852714
Logistic Regression Model Area under the curve for ROC is : 0.9949875553097345
Decision Tree Model Accuracy score is : 0.9602713178294574
Decision Tree Model Area under the curve for ROC is : 0.8901410398230089
\end{lstlisting} 
Actually, SVM realizes the best score for accuracy metric (97.2\%) and Bayes (98.9\%). Thus, the positives are well-learned as ROC graphic shows it in figure\ref{fig:rocenglish}.
 \begin{figure}
 	\centering
 	\includegraphics[width=0.9\linewidth]{CollectImages/rocEnglish}
 	\caption{Comparing models's AUC score based on English messages}
 	\label{fig:rocenglish}
 \end{figure}
\\

(b) Swahili model's score:

\begin{lstlisting}[style=stylejupyter]
In [345]: makeRocVisualize(BayesModelSw,SvmModelSw,LogRegModelSw,DecisionTreeSw,x_test_features_sw,y_test_sw,"(Swahili Data)")

#results 
Bayes Model Accuracy score is : 0.8695652173913043
Bayes Model Area under the curve for ROC is : 0.8999999999999999
SVM Model Accuracy score is : 0.8695652173913043
SVM Model Area under the curve for ROC is : 0.8666666666666667
Logistic Regression Model Accuracy score is : 0.8695652173913043
Logistic Regression Model Area under the curve for ROC is : 0.85
Decision Tree Model Accuracy score is : 0.9130434782608695
Decision Tree Model Area under the curve for ROC is : 0.6666666666666666
\end{lstlisting}  
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{CollectImages/rocSwahili}
	\caption{Comparing models's AUC score based on Swahili messages}
	\label{fig:rocswahili}
\end{figure}

For the case of Swahili messages, Decision Tree is the learning model with high score for accuracy (91.3\%) and Bayes for AUC (90\%)  (as even shown in the figure \ref{fig:rocswahili}).
\\

(c) French model's score :

 \begin{lstlisting}[style=stylejupyter]
In [355]: makeRocVisualize(BayesModelFr,SvmModelFr,LogRegModelFr,DecisionTreeFr,x_test_features_fr,y_test_fr,"(French Data)")

#results 
Bayes Model Accuracy score is : 0.9629990262901655
Bayes Model Area under the curve for ROC is : 0.9922135706340378
SVM Model Accuracy score is : 0.9776046738072055
SVM Model Area under the curve for ROC is : 0.994794563403782
Logistic Regression Model Accuracy score is : 0.9659201557935735
Logistic Regression Model Area under the curve for ROC is : 0.9910056312569522
Decision Tree Model Accuracy score is : 0.9591041869522883
Decision Tree Model Area under the curve for ROC is : 0.9096391824249166
\end{lstlisting}  
Models evaluated on French messages show that SVM is best predictor based on accuracy metric (97.2\%). In terms of AUC all represented models give 99\% except the Decision Tree Model as shown in ROC (fig. \ref{fig:rocfrench}) graphic. 
\\
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{CollectImages/rocFrench}
	\caption{Comparing model's AUC score based on French messages}
	\label{fig:rocfrench}
\end{figure}
\\
The all results models being combined show the better one according to the language data. The prompt view is shown in the table and in the figure \ref{modelclassificationscore}
\begin{table}[h]
\centering
\begin{tabular}{cccccc}
	\hline 
	\hline
	Languages& Metrics  & Naive & SVM  & LR & DT   \\
	\hline
	\hline
	Swahili  &	Accuracy &	86.96\% &	86.96\% &	86.96\% &	91.30\%\\
	
	& AUC	&90.00\%	& 86.67\%	& 85.00\%	&66.67\% \\
			
	French	& Accuracy& 	96.30\% & 	97.76\% & 	96.59\% & 	90.96\% \\
			 & 	AUC & 	99.22\% & 	99.48\%	& 99.10\%& 	90.96\% \\
	English &	Accuracy&	96.41\%&	97.29\%	&95.74\%	&96.03\% \\
	
	& AUC	& 98.93\% &	99.73\%	& 99.50\% &	89.01\%	\\	
	\hline		
	\hline
\end{tabular}
\caption{Models score (Accuracy vs AUC)}

\end{table}

\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{CollectImages/modelClassificationScore}
	\caption{Models classification's scores}
	\label{fig:modelclassificationscore}
\end{figure} 
\subsection{Optimization with Ensemble models}
Actually, the graphic results (fig.\ref{fig:modelclassificationscore}) shows that the every models have its characteristics allowing to be convenient to certain data, thus combining all for more optimization can be appealing by taking care of over-fitting. Therefore, two means are used :\\

(a) \textit{VotingClassifier} : \\

- English messages :
\begin{lstlisting}[style=stylejupyter]
In [118]: EnsembleModelVotingEn= 			 VotingClassifier(estimators=[('Bayes', BayesModelSw), ('SVM', SvmModelSw), ('LR', LogRegModelSw),('DecisionTreSw',DecisionTreSw)], voting='soft') 

In [119]:EnsembleModelVotingEn.fit(x_train_features_en, y_train_en) 
In [124]:print(accuracy_score(y_test_en, EnsembleModelVotingEn.predict(x_test_features_en)))
#Output:
0.9796511627906976
\end{lstlisting} 

- French messages :
\begin{lstlisting}[style=stylejupyter]
In [121]: EnsembleModelVotingFr = VotingClassifier(estimators=[('Bayes', BayesModelFr), ('SVM', SvmModelFr), ('LR', LogRegModelFr),('DecisionTreSw',DecisionTreeFr)], voting='soft')

In [122]:EnsembleModelVotingFr.fit(x_train_features_fr, y_train_fr)
In [124]:print(accuracy_score(y_test_fr, EnsembleModelVotingFr.predict(x_test_features_fr)))
#Output:
0.9805258033106135
\end{lstlisting}  

- Swahili messages : 
\begin{lstlisting}[style=stylejupyter]
In [124]: EnsembleModelVotingSw = VotingClassifier(estimators=[('Bayes', BayesModelSw), ('SVM', SvmModelSw), ('LR', LogRegModelSw),('DecisionTreSw',DecisionTreeSw)], voting='soft')
In [125]:EnsembleModelVotingSw.fit(x_train_features_sw, y_train_sw)
In [126]:print(accuracy_score(y_test_sw, EnsembleModelVotingSw.predict(x_test_features_sw)))
#Output:
0.8695652173913043
\end{lstlisting}  

(b) Using boost Method (GridSearch) 
Actually, by summarizing the scores metrics achieved are equals in terms of application, much more for the Swahili messages which lead to lower result than others. The main reason of that, is the a few number of messages comparing to others. Thus, by \textit{gridsearch}, we tested if the accuracy can not once more increases.
\begin{lstlisting}[style=stylejupyter]
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV 
	
gb_classifier = GradientBoostingClassifier() 
	
param_grid = {
	'n_estimators': [10, 50, 100],  # Number of boosting stages to be used
	'learning_rate': [0.01, 0.1, 0.2],  # Step size shrinking to prevent overfitting
	'max_depth': [3, 4, 5]  # Maximum depth of individual trees
	}
	
grid_search = GridSearchCV(gb_classifier, param_grid, cv=5, scoring='accuracy')
\end{lstlisting}

\begin{lstlisting}[style=stylejupyter]
In [131]: grid_search.fit(x_train_features_sw, y_train_sw) 

#get the best parameters 
In [132]: best_gb_classifier = grid_search.best_estimator_ 
best_gb_classifier
Out [132]: 
	GradientBoostingClassifier(learning_rate=0.01) 
	
##best gb model:
In [380]: best_gb_classifier = grid_search.best_estimator_ 
     y_test_en_pred = best_gb_classifier.predict(x_test_features_sw)
\end{lstlisting} 

\begin{lstlisting}[style=stylejupyter]
	y_test_en_pred = best_gb_classifier.predict(x_test_features_sw)
	
	accuracy = accuracy_score(y_test_sw, y_test_en_pred)
	print("Accuracy:", accuracy) 
	
	#output : 
	Accuracy: 0.8695652173913043
\end{lstlisting}
Great! The score is the same like the one yielded by the \textit{votingClassifier}  

\subsection{Model Deployment}
Concerning the current work, at this stage we converted the models into a files usable in production area. We considered all voting models are for each language because the scores they yielded, and dumped them in files.
\subsubsection{Production With Joblib}
\begin{lstlisting}[style=stylejupyter]
import joblib
#Models for preprocessing
joblib.dump(featureExtractionEn, 'EnglishFeatureModel.pkl')
joblib.dump(featureExtractionFr, 'FrenchFeatureModel.pkl')
joblib.dump(featureExtractionSw, 'SwahiliFeatureModel.pkl') 

#Models predictions
joblib.dump(EnsembleModelVotingEn, 'EnglishModelPrediction.pkl')
joblib.dump(EnsembleModelVotingFr, 'FrenchModelPrediction.pkl')
joblib.dump(EnsembleModelVotingSw, 'SwahiliModelPrediction.pkl')
\end{lstlisting} 
We have to convert any input into numeric by The \textit{featureExtraction} model from \textit{TdifVectorizer} and then 


\section{Presentation and Discussion of Results} 
For sure, this step is the next party of the previous, we just show how the model generated during the deployment stage is integrated in a software for really performing it tasks. As basics, the following step delves into precepts of interaction. 
\subsection{Predicting system }
Performing predictions which taking account of languages, required this project to use a Python detect\_langs library. Hence, the following function allows us to judge if the input message should be considered as a French, English or Swahili text every time it doesn't match one those languages, it automatically considered as an Swahili text.
\begin{lstlisting}[style=stylejupyter]
from langdetect import detect_langs 
def detectLanguage(text) :
	result = detect_langs(text)
	for language in result:
	if language.lang == 'fr':
	return 'fr'
	elif language.lang == 'en':
	return 'en'
	else:
	return 'sw'
\end{lstlisting} 

Next, the following function named \textit{predictMessageState} performs the prediction task and returns the appealing answer including the numeric  value and the probability behind that prediction. Indeed, for the numeric value understood as a boolean, every time it is 0, it means that the message is ham, when it is 1 it stands for that the message is a spam. 
\begin{lstlisting}[style=stylejupyter]
def predictMessageState(input_text):
	if detectLanguage(input_text)=='en':
	input_vector = featureExtractionEn.transform([input_text])
	prediction = EnsembleModelVotingEn.predict(input_vector)[0]
	predict_proba = EnsembleModelVotingEn.predict_proba(input_vector)[0]
	if detectLanguage(input_text)=='fr':
	input_vector = featureExtractionFr.transform([input_text])
	prediction = EnsembleModelVotingFr.predict(input_vector)[0]
	predict_proba = EnsembleModelVotingFr.predict_proba(input_vector)[0]
	else:
	input_vector = featureExtractionSw.transform([input_text])
	prediction = best_gb_classifier.predict(input_vector)[0]
	predict_proba = best_gb_classifier.predict_proba(input_vector)[0] 
	
	return prediction, predict_proba
\end{lstlisting}  

Then, the present snipped code, is the main or starter of the operations: 
\begin{lstlisting}[style=stylejupyter]
input_text = input("Enter your mail here: ") 
predictMessageState(input_text) 
##results
Enter your mail here: jambo kaka
(0, array([0.62955115, 0.37044885]))
\end{lstlisting}

\subsection{Model interaction in a Platform for detection}
Just as the user was completing the form for submitting messages that look like or are spams, at  this stage. When he receives an message in his phones transmitted by a mobile network operator, he has to check its legitimacy by copping it and submitting through the following steps :
\begin{figure}[h]
	\centering
	\begin{subfigure}{3.5cm}
		\centering
		\includegraphics[width=1\linewidth]{CollectImages/mobileSpamTest.png}
		\caption{}
		\label{fig:formCompleted}
	\end{subfigure}
	\begin{subfigure}{3.5cm}
		\centering
		\includegraphics[width=1\linewidth]{CollectImages/formCompleted.png}
		\caption{}
		\label{fig:mobilespamtest}
	\end{subfigure} 
	\begin{subfigure}{3.5cm}
		\centering
		\includegraphics[width=1\linewidth]{CollectImages/mobilePoneTestAnswer}
		\caption{}
		\label{fig:mobileponetestanswer}
	\end{subfigure}  
\end{figure}

At the first stage (a), the user has to click on the test link, then completing the form and sending it  at (b), finally waiting for the response of the server.

\subsection{Model's system with APIs}
For network operators or other software developers  who would like to integrate the model functionalities or networks operators,  they have to follow the link delivered end-point end-point. The can consume the services as insomnia can do while testing:
\begin{figure}[h]
	\centering
	\includegraphics[width=15.8cm, height=2.9cm]{CollectImages/insomniaTest}
	\caption{Testing the end-point for api Services}
	\label{fig:insomniatest}
\end{figure} 
As the image shows, the answer is 1 meaning that the message is spam based on the probability of 58\%

\section{Theoretical and Practical Contributions}
Creating a model capable of handling situations involving panic, disturbance, and financial loss can be a formidable challenge. This work serves as an assistant for mobile phone users, helping them assess the legitimacy of received messages and make informed decisions. Consequently, individuals who have experienced significant losses through various means now have a valuable resource.\\

Moreover, network operators who record uncontrolled financial losses and customer defections can enhance their support systems by incorporating a feature that provides information on numbers involved in such activities. Alongside their existing techniques, this model can be integrated into their message transmission systems to filter processed messages for further investigation and enhance overall security.\\

Additionally, this solution offers a valuable addition for other developers utilizing the messaging system. By obtaining the necessary credentials, they can integrate this solution into their services, aiming to provide modern and high-quality service delivery.
\section{Study Limitations and Future research paths} 
Although this work comes with several advantages, it also has its limitations. The solution it offers isn't universal, as it's primarily tailored to a specific region, especially the eastern part of the Democratic Republic of Congo.\\

Furthermore, a portion data, which is a fundamental component of any machine learning algorithms, was collected from an unofficial source, namely \textit{Kaggle}. This data doesn't consider the local interests and language cultures, as it predominantly consists of content in French and English. Additionally, the other data sources were limited in quantity, making it challenging to provide accurate estimations that represent the entire population.\\

Moreover, the solution is designed to work with only three languages. This means that messages containing a mixture of languages might pose challenges for the model in terms of accurate classification. There is a need for a more advanced multilingual model classifier to address this issue for instance by utilizing advanced techniques in artificial intelligence.
 
\section{Summary}
This chapter provides practical insights into the methodology. It focuses on the practical applications of this methodology within machine learning systems. It outlines the process of data collection, including the sources of data, the providers, and the methods used to acquire the data necessary to initiate the machine learning model.\\

Additionally, it offers various techniques and code snippets to aid in data analysis, ultimately leading to the development of a functional model that can be deployed effectively.\\

Furthermore, this chapter explores the potential impact of the project on individuals, software developers, and network operators, offering practical insights into how it can be integrated into their daily work and life.\\

In conclusion, it highlights the project's strengths and limitations and invites other researchers to further investigate and develop highly accurate spam prediction models.\\

\chapter*{General Conclusion} 
This study primarily focused on the detection of spam messages within network operator environments. Throughout its course, it highlighted the numerous issues that mobile phones users encounter. Thus, they are occasionally vulnerable to threats, disturbances, and monetary losses, usually caused by scammers exploiting the messaging system or mobile devices themselves.\\

To address these problems, this project proposes potential solutions. It not only educates users on how to assess and identify malicious messages independently but also introduces a predictive model capable of assisting users in the fight against illegitimate messages. Obviously, this work emphasizes that users also have a crucial role to play in this endeavor.\\

Implementing these solutions requires the use of specific techniques and methods that align with professional standards. This involves using programming languages such as HTML, CSS, and JavaScript to develop user-friendly interfaces. For the development and management of machine learning models and message processing, Python and its libraries (including Pandas, NumPy, Matplotlib, and Scikit-Learn) were employed.\\

Data, often considered the heart of the model, received special attention. They underwent thorough static analysis, preparation, and visualization to ensure a clear understanding and prevent biased results. Once the data were ready, they were used to train machine learning models, employing various algorithms such as Naive Bayes, Logistic Regression, Support Vector Machine, Decision Trees, and ensemble models. These models were trained on datasets in Swahili, French, and English. After training, the models were integrated into a production environment following their deployment.\\

In terms of practical solution, the current work introduces a system that offers a server interaction system with APIs for software developers who wish to incorporate the model's capabilities into their applications. It also addresses the needs of network operators by enhancing monitoring and surveillance tasks.\\

However, the project has its limitations. The quantity of Swahili messages was limited, which could potentially lead to a biased model. Additionally, gaining access to network operator messages can be a challenging process due to legal procedures. Furthermore, the model only supports three languages, while the real-world scenario involves more diversity in language use. Messages often contain mixed languages, and some threats target system architecture rather than message content. These concerns, along with others, provide ample opportunities for future research.
 
 

\bibliographystyle{plain}
\bibliography{bibliofile.bib}

\backmatter
\appendix


\chapter*{Annexes}
\addcontentsline{toc}{chapter}{Annexes}

\section{Collect message spams} 
In the website running , using the current link \footnote{Spam Collect app : \url{http://spam.kivu-cs.org/}} sent on the a form where its possible to complete both the number of the spam message's sender and the message.The form receiving the data and sent it to the management system where they are stored in the database if they are valid. This is th illustrative image:
\begin{figure}[h]
	\centering
	\includegraphics[width=0.4\linewidth]{CollectImages/spamForm}
	\caption{Collect data by the website's form}
	\label{fig:spamform}
\end{figure} 
\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{CollectImages/collectData}
	\caption{Download collected data}
	\label{fig:collectdata} 	
\end{figure}
\\

\section{Code for the api interaction}
\begin{lstlisting}[style=stylejupyter]
from django.shortcuts import render
from django.http import HttpResponse
from django.http import JsonResponse
from django.contrib.auth import authenticate, login
from django.shortcuts import render, redirect
from django.core.paginator import Paginator
import csv
from django.contrib.auth import logout
from .forms import SpamMessageForm, contactForm, TestMessageForm
from .models import spamsMessage 
from django.shortcuts import render
from django.core.paginator import Paginator
from django.http import JsonResponse
import json
from django.conf import settings
from langdetect import detect_langs 
import joblib 
import numpy as np

from rest_framework.decorators import api_view
from rest_framework.response import Response
from rest_framework import status
from .models import TestMessage  # Replace with your actual model
from .serializers import TestMessageSerializer  # Create a serializer for your model

from  django.core.mail import EmailMessage, send_mail, get_connection

EnsembleModelVotingEn = joblib.load("/home/christianresearcher/Documents/Courses/L2/Dissertation/spamDetectionApp/project/static/models/EnglishModelPrediction.pkl")
EnsembleModelVotingFr = joblib.load('/home/christianresearcher/Documents/Courses/L2/Dissertation/spamDetectionApp/project/static/models/FrenchModelPrediction.pkl')
EnsembleModelVotingSw = joblib.load('/home/christianresearcher/Documents/Courses/L2/Dissertation/spamDetectionApp/project/static/models/SwahiliModelPrediction.pkl')

featureExtractionEn = joblib.load("/home/christianresearcher/Documents/Courses/L2/Dissertation/spamDetectionApp/project/static/models/EnglishFeatureModel.pkl") 
featureExtractionFr = joblib.load("/home/christianresearcher/Documents/Courses/L2/Dissertation/spamDetectionApp/project/static/models/FrenchFeatureModel.pkl")
featureExtractionSw = joblib.load("/home/christianresearcher/Documents/Courses/L2/Dissertation/spamDetectionApp/project/static/models/SwahiliFeatureModel.pkl")


def index_view(request):
	return render(request, "indexu.html")


def download_csv(request):
	response = HttpResponse(content_type="text/csv")
	response["Content-Disposition"] = 'attachment; filename="messages.csv"'
	
	writer = csv.writer(response)
	writer.writerow(["id", "Contact", "Message"])
	messages = spamsMessage.objects.all().order_by("createat")
	
	for message in messages:
	writer.writerow(
	[
	message.createat,
	message.contact,
	message.message,
	]
	)
	return response

def statistic_views(request):
	query_set = spamsMessage.objects.all()
	content_query = request.GET.get("message")
	contact_query = request.GET.get("contact")
	if contact_query:
	query_set = query_set.filter(contact__icontains=content_query)
	if content_query:
	query_set = query_set.filter(message__icontains=content_query)
	paginator = Paginator(query_set, 5)  # paginate by 10 messages per page
	page_number = request.GET.get("page")
	page_obj = paginator.get_page(page_number)
	context = {
	"messages": page_obj,
	"contact": contact_query,
	"message": content_query,
	}
	return render(request, "statistics.html", context)


def login_view(request):
	if request.method == "POST":
	username = request.POST.get("username")
	password = request.POST.get("password")
	if username is not None and password is not None:
	user = authenticate(request, username=username, password=password)
	if user is not None:
	login(request, user)
	return redirect("/")
	error = "Invalid username or password."
	return render(request, "loginu.html", {"error": error})
	else:
	return render(request, "loginu.html")

def logout_view(request):
	logout(request)
	return redirect("login")


@api_view(['POST'])
def api_test_message_view(request):
	if request.method == "POST":
	serializer = TestMessageSerializer(data=request.data)
	if serializer.is_valid():
	serializer.save()
	message = request.data.get('message')
	response, proba = predictMessageState(message)
	answer = {
	'message': message,
	'label': int(response),
	'probability': proba.tolist()
	}
	return Response(answer, status=status.HTTP_200_OK)
	else:
	return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)
	else:
	return Response({'error': 'Only POST requests are allowed.'}, status=status.HTTP_405_METHOD_NOT_ALLOWED)



def testMessageViewCollect(request):
	if request.method == "POST":
	form = TestMessageForm(request.POST)
	if form.is_valid():
	form.save()
	message =  request.POST.get('message')
	response, proba  = predictMessageState(message) 
	answer={} 
	answer['message']= message
	answer['label']= int(response)
	answer['probability']= proba.tolist()  
	
	return JsonResponse({"message": answer})
	else:
	return JsonResponse({"message": "error"})
	else:
	return JsonResponse({"error": "Only POST requests are allowed."})

def testMessageView(request):
	return render(request, "test.html")

def predictMessageState(input_text):
	if detectLanguage(input_text)=='en':
	input_vector = featureExtractionEn.transform([input_text])
	prediction = EnsembleModelVotingEn.predict(input_vector)[0]
	predict_proba = EnsembleModelVotingEn.predict_proba(input_vector)[0]
	if detectLanguage(input_text)=='fr':
	input_vector = featureExtractionFr.transform([input_text])
	prediction = EnsembleModelVotingFr.predict(input_vector)[0]
	predict_proba = EnsembleModelVotingFr.predict_proba(input_vector)[0]
	else:
	input_vector = featureExtractionSw.transform([input_text])
	prediction = EnsembleModelVotingSw.predict(input_vector)[0]
	predict_proba = EnsembleModelVotingSw.predict_proba(input_vector)[0] 
	
	return prediction, predict_proba

def detectLanguage(text) :
	result = detect_langs(text)
	for language in result:
	if language.lang == 'fr':
	return 'fr'
	elif language.lang == 'en':
	return 'en'
	else:
	return 'sw'
\end{lstlisting}
The serializers
\begin{lstlisting}[style=stylejupyter]
from rest_framework import serializers
from .models import TestMessage

class TestMessageSerializer(serializers.ModelSerializer):
class Meta:
model = TestMessage
fields = ('message', 'createat')  
\end{lstlisting}
Models
\begin{lstlisting}[style=stylejupyter] 
from django.db import models
from django.core.files import File
from io import BytesIO

class TestMessage(models.Model):
message = models.TextField(null=False)
createat = models.DateField(auto_now_add=True)

\end{lstlisting}
\backmatter
\appendix	
	
	
	
	
	
	
	
\end{document}